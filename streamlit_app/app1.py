import os
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import json
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configure base directory for file paths
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"

# Set page configuration
st.set_page_config(
    page_title="Healthcare Fraud Detection - Business Intelligence Dashboard",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS for better styling
st.markdown("""
<style>
.metric-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 1rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin: 0.5rem 0;
}
.info-card {
    background-color: #f8f9fa;
    padding: 1.5rem;
    border-radius: 10px;
    border-left: 4px solid #007bff;
    margin: 1rem 0;
}
.success-card {
    background-color: #d4edda;
    padding: 1.5rem;
    border-radius: 10px;
    border-left: 4px solid #28a745;
    margin: 1rem 0;
}
.warning-card {
    background-color: #fff3cd;
    padding: 1.5rem;
    border-radius: 10px;
    border-left: 4px solid #ffc107;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
<div style="text-align: center; padding: 2rem 0; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); margin: -1rem -1rem 2rem -1rem; color: white;">
    <h1 style="font-size: 3rem; margin: 0;">ğŸ¥ Healthcare Provider Fraud Detection</h1>
    <p style="font-size: 1.2rem; margin: 0.5rem 0 0 0;">Business Intelligence Dashboard for Provider Risk Assessment</p>
</div>
""", unsafe_allow_html=True)






# Load data function
@st.cache_data
def load_data():
    """Load all necessary data files"""
    data = {}
    errors = []
    try:
        # Load main features data - this contains both features and labels
        features_path = DATA_DIR / "features_full.parquet"
        if features_path.exists():
            data['features'] = pd.read_parquet(features_path)
            # Ensure Provider column is string
            data['features']['Provider'] = data['features']['Provider'].astype(str)
        else:
            errors.append(f"Missing: {features_path}")
        
        # Load EDA metrics
        eda_metrics_path = DATA_DIR / "eda" / "eda_metrics.json"
        if eda_metrics_path.exists():
            with open(eda_metrics_path, 'r') as f:
                data['eda_metrics'] = json.load(f)
                
        # Load counts summary
        counts_path = DATA_DIR / "eda" / "counts_summary.json"
        if counts_path.exists():
            with open(counts_path, 'r') as f:
                data['counts_summary'] = json.load(f)
                
        # Load EDA summary
        eda_summary_path = DATA_DIR / "eda" / "eda_summary.json"
        if eda_summary_path.exists():
            with open(eda_summary_path, 'r') as f:
                data['eda_summary'] = json.load(f)
        
        # Load SHAP feature importance
        shap_path = DATA_DIR / "validation_sets" / "shap_feature_importance.csv"
        if shap_path.exists():
            data['shap_importance'] = pd.read_csv(shap_path)
        
        # Load provider example (Provider 13)
        provider_path = DATA_DIR / "validation_sets" / "provider13_features.csv"
        if provider_path.exists():
            data['provider_example'] = pd.read_csv(provider_path)
        
        # Load top outliers from EDA
        outliers_path = DATA_DIR / "eda" / "eda_top_outliers.csv"
        if outliers_path.exists():
            data['top_outliers'] = pd.read_csv(outliers_path)
            
    except Exception as e:
        st.error(f"Critical error loading data: {e}")
        return None, [str(e)]
    
    return data, errors

# Load model and metrics
@st.cache_resource
def load_model_info():
    """Load model and metrics"""
    model_info = {}
    
    try:
        # Load metrics - using correct path
        metrics_path = DATA_DIR / "metrics" / "XGBost_metrics.json"
        if metrics_path.exists():
            with open(metrics_path, 'r') as f:
                model_info['metrics'] = json.load(f)
            
    except Exception as e:
        pass  # Silently handle errors - app will gracefully degrade
        
    return model_info

# Load data
data, data_errors = load_data()

# Load model info
model_info = load_model_info()

# Create tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“¦ Dataset Overview",
    "ğŸ—ï¸ Feature Analysis", 
    "ğŸ¤– Model Performance",
    "ğŸ“Š SHAP Explainability",
    "ğŸ•µï¸ Fraud Risk Predictor",
    "ğŸ’¼ Business Impact"
])

# Tab 1: Dataset Overview
with tab1:
    st.header("ğŸ“¦ Dataset Overview")
    
    # Project Description
    st.markdown("""
    <div class="info-card">
    <h3>ğŸ¯ Project Mission</h3>
    <p><strong>Provider-level fraud detection using Medicare claims data.</strong></p>
    <p>This system analyzes healthcare provider patterns to identify potentially fraudulent activities, 
    protecting healthcare systems from billions in annual losses while ensuring legitimate providers 
    continue serving patients effectively.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Data Sources & Schema
    st.subheader("ğŸ“‹ Data Sources & Schema")
    
    # Add data source reference with enhanced styling
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 15px; color: white; margin-bottom: 1.5rem;">
    <h4 style="margin: 0; color: white;">ğŸ¥ Data Source Information</h4>
    <p style="margin: 0.5rem 0;"><strong>Coverage:</strong> Healthcare providers, beneficiaries (patients), and claims (inpatient/outpatient)</p>
    <p style="margin: 0; font-size: 0.9rem;"><strong>Dataset Link:</strong> <a href="https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis/code" target="_blank" style="color: #FFE4B5;">Healthcare Provider Fraud Detection Analysis - Kaggle</a></p>
    </div>
    """, unsafe_allow_html=True)
    
    # Create dataset overview table using real EDA data
    if data and 'eda_summary' in data:
        # Extract real dimensions from EDA summary
        eda_summary = data['eda_summary']
        train_data = eda_summary.get('Train.csv', {})
        bene_data = eda_summary.get('Train_Beneficiarydata.csv', {}) 
        inp_data = eda_summary.get('Train_Inpatientdata.csv', {})
        out_data = eda_summary.get('Train_Outpatientdata.csv', {})
        
        # Create dataset table with real data
        dataset_overview = pd.DataFrame({
            'Dataset': [
                'ğŸ·ï¸ Training Data',
                'ğŸ‘¥ Beneficiary Data', 
                'ğŸ¥ Inpatient Claims',
                'ğŸš‘ Outpatient Claims'
            ],
            'Description': [
                'Provider fraud labels',
                'Patient demographics',
                'Hospital admissions', 
                'Ambulatory care'
            ],
            'Rows': [
                f"{train_data.get('num_rows', 0):,}",
                f"{bene_data.get('num_rows', 0):,}",
                f"{inp_data.get('num_rows', 0):,}",
                f"{out_data.get('num_rows', 0):,}"
            ],
            'Columns': [
                train_data.get('num_columns', 0),
                bene_data.get('num_columns', 0),
                inp_data.get('num_columns', 0),
                out_data.get('num_columns', 0)
            ],
            'Source File': [
                'Train.csv',
                'Train_Beneficiarydata.csv',
                'Train_Inpatientdata.csv', 
                'Train_Outpatientdata.csv'
            ]
        })
        
        # Display the table with better formatting
        st.markdown("#### ğŸ“Š Dataset Overview")
        st.dataframe(
            dataset_overview, 
            use_container_width=True,
            hide_index=True,
            column_config={
                "Dataset": st.column_config.TextColumn("Dataset", width="large"),
                "Description": st.column_config.TextColumn("Description", width="large"),
                "Rows": st.column_config.TextColumn("Rows", width="small"),
                "Columns": st.column_config.NumberColumn("Columns", width="small"),
                "Source File": st.column_config.TextColumn("Source File", width="medium")
            }
        )
        
        # Add summary metrics with beautiful colored cards
        st.markdown("#### ğŸ“ˆ Dataset Summary")
        total_rows = (train_data.get('num_rows', 0) + bene_data.get('num_rows', 0) + 
                     inp_data.get('num_rows', 0) + out_data.get('num_rows', 0))
        total_claims = inp_data.get('num_rows', 0) + out_data.get('num_rows', 0)
        
        # Create beautiful metric cards
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #FF6B6B, #FF8E8E); padding: 1.5rem; border-radius: 15px; text-align: center; color: white;">
                <h3 style="margin: 0; font-size: 2rem;">{total_rows:,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ“Š Total Records</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.9;">Across all datasets</p>
            </div>
            """, unsafe_allow_html=True)
            
        with col2:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #4ECDC4, #5DECDE); padding: 1.5rem; border-radius: 15px; text-align: center; color: white;">
                <h3 style="margin: 0; font-size: 2rem;">{total_claims:,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ¥ Total Claims</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.9;">Inpatient + Outpatient</p>
            </div>
            """, unsafe_allow_html=True)
            
        with col3:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #45B7D1, #5BC7E1); padding: 1.5rem; border-radius: 15px; text-align: center; color: white;">
                <h3 style="margin: 0; font-size: 2rem;">{train_data.get('num_rows', 0):,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ‘¥ Providers</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.9;">Healthcare providers</p>
            </div>
            """, unsafe_allow_html=True)
            
        with col4:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #A8E6CF, #B8F6DF); padding: 1.5rem; border-radius: 15px; text-align: center; color: #2C3E50;">
                <h3 style="margin: 0; font-size: 2rem;">{bene_data.get('num_rows', 0):,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ‘¤ Beneficiaries</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.8;">Unique patients</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Schema details in expanders
        with st.expander("ğŸ” View Schema Details"):
            schema_tab1, schema_tab2, schema_tab3, schema_tab4 = st.tabs([
                "Training Data", "Beneficiary Data", "Inpatient Claims", "Outpatient Claims"
            ])
            
            with schema_tab1:
                st.write("#### Training Data Schema")
                st.table(pd.DataFrame({
                    "Column": train_data.get("columns", []),
                    "Type": ["string", "integer (0/1)"]
                }))
                
            with schema_tab2:
                st.write("#### Beneficiary Data Schema (First 10 Columns)")
                if bene_data.get("columns"):
                    columns = bene_data.get("columns")[:10]
                    types = ["string", "datetime", "datetime", "string", "string", "string", "integer (0/1)", "string", "string", "string"][:len(columns)]
                    st.table(pd.DataFrame({"Column": columns, "Type": types}))
                    
            with schema_tab3:
                st.write("#### Inpatient Claims Schema (First 10 Columns)")
                if inp_data.get("columns"):
                    columns = inp_data.get("columns")[:10]
                    types = ["string", "string", "string", "datetime", "datetime", "datetime", "datetime", "string", "string", "string"][:len(columns)]
                    st.table(pd.DataFrame({"Column": columns, "Type": types}))
                    
            with schema_tab4:
                st.write("#### Outpatient Claims Schema (First 10 Columns)")
                if out_data.get("columns"):
                    columns = out_data.get("columns")[:10]
                    types = ["string", "string", "string", "datetime", "datetime", "string", "string", "string", "float", "string"][:len(columns)]
                    st.table(pd.DataFrame({"Column": columns, "Type": types}))
        
        # Missing Data Visualizations (moved from cleaning section)
        with st.expander("ğŸ“Š Missing Data Analysis"):
            st.markdown("**Data Quality Assessment: Missing Values Analysis**")
            st.markdown("*Understanding data completeness across all datasets is crucial for reliable fraud detection.*")
            
            vis_col1, vis_col2, vis_col3 = st.columns(3)
            
            with vis_col1:
                st.markdown("**ğŸ‘¥ Beneficiary Data**")
                bene_missing_path = DATA_DIR / "eda" / "eda_missing_train_bene.png"
                if bene_missing_path.exists():
                    st.image(str(bene_missing_path), caption="Missing Values in Beneficiary Dataset")
                else:
                    st.info("Beneficiary missing data plot not available")
                    
            with vis_col2:
                st.markdown("**ğŸ¥ Inpatient Claims**")
                inp_missing_path = DATA_DIR / "eda" / "eda_missing_train_inp.png"
                if inp_missing_path.exists():
                    st.image(str(inp_missing_path), caption="Missing Values in Inpatient Claims")
                else:
                    st.info("Inpatient missing data plot not available")
                    
            with vis_col3:
                st.markdown("**ğŸš‘ Outpatient Claims**")
                out_missing_path = DATA_DIR / "eda" / "eda_missing_train_out.png"
                if out_missing_path.exists():
                    st.image(str(out_missing_path), caption="Missing Values in Outpatient Claims")
                else:
                    st.info("Outpatient missing data plot not available")
            
            # Summary insights
            st.markdown("""
            **ğŸ’¡ Key Insights from Missing Data Analysis:**
            - **Beneficiary Data**: Excellent completeness (99%+) with only DOD missing for living patients
            - **Inpatient Claims**: Core medical fields complete, some optional procedure codes missing
            - **Outpatient Claims**: Similar pattern to inpatient with high completeness in essential fields
            - **Overall Impact**: Missing data patterns are logical and don't compromise fraud detection capability
            """)
            
    else:
        # Fallback - show error and try alternative data sources
        st.error("âŒ Unable to load dataset overview from EDA summary")
        
        # Try to show basic info from other sources if available
        if data and 'counts_summary' in data:
            st.warning("ğŸ“Š Showing basic counts from alternative data source:")
            counts = data['counts_summary']
            
            basic_overview = pd.DataFrame({
                'Metric': [
                    'Providers in Training Data',
                    'Providers in Inpatient Claims',
                    'Providers in Outpatient Claims',
                    'Beneficiaries in Data'
                ],
                'Count': [
                    f"{counts.get('Providers in train', 0):,}",
                    f"{counts.get('Providers in inpatient', 0):,}",
                    f"{counts.get('Providers in outpatient', 0):,}",
                    f"{counts.get('Beneficiaries in beneficiary data', 0):,}"
                ]
            })
            
            st.dataframe(basic_overview, use_container_width=True, hide_index=True)
        else:
            st.error("âŒ No dataset information available")
    
    # Data Cleaning & Transformation
    st.subheader("ğŸ§¹ Data Cleaning & Transformation")
    
    # Enhanced intro with gradient styling
    st.markdown("""
    <div style="background: linear-gradient(135deg, #FFA726, #FFB74D); padding: 1.5rem; border-radius: 15px; color: white; margin-bottom: 1.5rem;">
    <h4 style="margin: 0; color: white;">âœ¨ Data Quality Enhancement</h4>
    <p style="margin: 0.5rem 0;">Comprehensive data cleaning and transformation pipeline ensuring high-quality, analysis-ready datasets</p>
    <p style="margin: 0; font-size: 0.9rem; opacity: 0.9;">All transformations follow best practices for healthcare data processing and ML model preparation</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Load cleaning data if available
    cleaning_checks_path = DATA_DIR / "metadata" / "cleaning_checks.json"
    if cleaning_checks_path.exists():
        try:
            with open(cleaning_checks_path, 'r') as f:
                cleaning_checks = json.load(f)
                
            clean_data_table = pd.DataFrame({
                'Data Transformation': [
                    'Date Columns',
                    'Target Encoding',
                    'RenalDisease Indicator',
                    'Missing Values: DeductibleAmtPaid',
                    'Missing Values: DiagnosisGroupCode',
                    'Missing Values: ClmAdmitDiagnosisCode',
                    'Added Derived Columns'
                ],
                'Action Taken': [
                    'Converted to datetime',
                    'Yes â†’ 1, No â†’ 0',
                    'Y â†’ 1, 0 â†’ 0',
                    'Filled with 0',
                    'Filled with -1',
                    'Filled with "Unknown"',
                    'Added HasDied flag'
                ],
                'Business Reason': [
                    'Enable time-series analysis and correct duration calculations',
                    'Standard binary encoding for ML modeling',
                    'Standard binary encoding for ML modeling',
                    'No deductible paid is logically 0',
                    'Missing diagnostic group needs explicit marker',
                    'Missing diagnosis needs explicit marker',
                    'Important mortality indicator for risk modeling'
                ]
            })
            
            st.table(clean_data_table)
            
            # Data integrity metrics with enhanced styling
            st.markdown("#### ğŸ” Data Integrity Validation")
            
            # Create beautiful integrity check cards
            integrity_col1, integrity_col2 = st.columns(2)
            
            unmatched_inp_providers = cleaning_checks.get("inpatient unmatched providers", 0)
            unmatched_out_providers = cleaning_checks.get("outpatient unmatched providers", 0)
            unmatched_inp_bene = cleaning_checks.get("inpatient unmatched BeneID", 0)
            unmatched_out_bene = cleaning_checks.get("outpatient unmatched BeneID", 0)
            
            with integrity_col1:
                # Provider integrity
                provider_status = "âœ… Perfect" if (unmatched_inp_providers + unmatched_out_providers) == 0 else "âŒ Issues Found"
                provider_color = "linear-gradient(135deg, #52C41A, #73D13D)" if (unmatched_inp_providers + unmatched_out_providers) == 0 else "linear-gradient(135deg, #FF4D4F, #FF7875)"
                
                st.markdown(f"""
                <div style="background: {provider_color}; padding: 1.5rem; border-radius: 15px; text-align: center; color: white; margin-bottom: 1rem;">
                    <h4 style="margin: 0;">ğŸ¥ Provider Integrity</h4>
                    <p style="margin: 0.5rem 0; font-size: 1.2rem; font-weight: bold;">{provider_status}</p>
                    <p style="margin: 0; font-size: 0.9rem; opacity: 0.9;">Inpatient: {unmatched_inp_providers} | Outpatient: {unmatched_out_providers}</p>
                </div>
                """, unsafe_allow_html=True)
                
            with integrity_col2:
                # Beneficiary integrity
                bene_status = "âœ… Perfect" if (unmatched_inp_bene + unmatched_out_bene) == 0 else "âŒ Issues Found"
                bene_color = "linear-gradient(135deg, #52C41A, #73D13D)" if (unmatched_inp_bene + unmatched_out_bene) == 0 else "linear-gradient(135deg, #FF4D4F, #FF7875)"
                
                st.markdown(f"""
                <div style="background: {bene_color}; padding: 1.5rem; border-radius: 15px; text-align: center; color: white; margin-bottom: 1rem;">
                    <h4 style="margin: 0;">ğŸ‘¤ Beneficiary Integrity</h4>
                    <p style="margin: 0.5rem 0; font-size: 1.2rem; font-weight: bold;">{bene_status}</p>
                    <p style="margin: 0; font-size: 0.9rem; opacity: 0.9;">Inpatient: {unmatched_inp_bene} | Outpatient: {unmatched_out_bene}</p>
                </div>
                """, unsafe_allow_html=True)
                                
        except Exception as e:
            st.error(f"Could not load cleaning data: {e}")
    else:
        st.info("Data cleaning summary not available")
    
    # Key EDA Insights
    st.subheader("ğŸ“Š Key EDA Insights")
    
    # Use real EDA metrics if available
    if data and 'eda_metrics' in data:
        eda_metrics = data['eda_metrics']
        
        # Enhanced metric boxes with better styling
        st.markdown("#### ğŸ¯ Fraud Distribution Overview")
        
        # Show fraud distribution metrics in attractive boxes
        fraud_pct = eda_metrics.get('fraud_percent', 9.35)
        legit_pct = eda_metrics.get('legit_percent', 90.65)
        high_risk_fraud_rate = eda_metrics.get('very_high_risk_fraud_rate', 34.00)
        
        # Create beautiful metric cards
        metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
        
        with metric_col1:
            st.markdown("""
            <div class="metric-card">
                <h3 style="margin: 0; color: white;">ğŸš¨ Fraudulent</h3>
                <h2 style="margin: 0.5rem 0; color: white;">{:.2f}%</h2>
                <p style="margin: 0; color: white; opacity: 0.9;">506 providers</p>
            </div>
            """.format(fraud_pct), unsafe_allow_html=True)
        
        with metric_col2:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #28a745 0%, #20c997 100%); padding: 1rem; border-radius: 10px; color: white; text-align: center; margin: 0.5rem 0;">
                <h3 style="margin: 0; color: white;">âœ… Legitimate</h3>
                <h2 style="margin: 0.5rem 0; color: white;">{:.2f}%</h2>
                <p style="margin: 0; color: white; opacity: 0.9;">4,904 providers</p>
            </div>
            """.format(legit_pct), unsafe_allow_html=True)
        
        with metric_col3:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #fd7e14 0%, #e83e8c 100%); padding: 1rem; border-radius: 10px; color: white; text-align: center; margin: 0.5rem 0;">
                <h3 style="margin: 0; color: white;">âš ï¸ High-Risk Rate</h3>
                <h2 style="margin: 0.5rem 0; color: white;">{:.1f}%</h2>
                <p style="margin: 0; color: white; opacity: 0.9;">Top quartile</p>
            </div>
            """.format(high_risk_fraud_rate), unsafe_allow_html=True)
        
        with metric_col4:
            fraud_multiplier = eda_metrics.get('avg_reimb_fraud', 584350) / eda_metrics.get('avg_reimb_legit', 53194)
            st.markdown("""
            <div style="background: linear-gradient(135deg, #6f42c1 0%, #e83e8c 100%); padding: 1rem; border-radius: 10px; color: white; text-align: center; margin: 0.5rem 0;">
                <h3 style="margin: 0; color: white;">ğŸ’° Risk Multiplier</h3>
                <h2 style="margin: 0.5rem 0; color: white;">{:.1f}x</h2>
                <p style="margin: 0; color: white; opacity: 0.9;">Higher reimb.</p>
            </div>
            """.format(fraud_multiplier), unsafe_allow_html=True)
        
        # Show reimbursement comparison with enhanced styling
        st.markdown("#### ğŸ’° Financial Impact Analysis")
        avg_fraud_reimb = eda_metrics.get('avg_reimb_fraud', 584350)
        avg_legit_reimb = eda_metrics.get('avg_reimb_legit', 53194)
        
        reimb_col1, reimb_col2, reimb_col3 = st.columns(3)
        with reimb_col1:
            st.metric(
                "ğŸ’¸ Avg Fraud Reimbursement", 
                f"${avg_fraud_reimb:,.0f}",
                delta=f"+{((avg_fraud_reimb/avg_legit_reimb - 1)*100):.0f}% vs legitimate"
            )
        with reimb_col2:
            st.metric(
                "ğŸ’³ Avg Legitimate Reimbursement", 
                f"${avg_legit_reimb:,.0f}",
                delta="Baseline"
            )
        with reimb_col3:
            total_fraud_impact = 506 * avg_fraud_reimb / 1_000_000  # In millions
            st.metric(
                "ğŸ“Š Total Fraud Exposure", 
                f"${total_fraud_impact:.1f}M",
                delta="Potential annual loss"
            )
        
        # Add key insight callout
        st.markdown("""
        <div class="warning-card">
        <h4>ğŸ” Key Financial Insight</h4>
        <p>Fraudulent providers receive <strong>{:.1f}x more reimbursement</strong> on average than legitimate providers, 
        representing a potential annual exposure of <strong>${:.1f} million</strong> from identified fraudulent providers alone.</p>
        </div>
        """.format(fraud_multiplier, total_fraud_impact), unsafe_allow_html=True)
        
        # Key EDA visualizations
        st.markdown("#### ğŸ“ˆ Key EDA Visualizations")
        
        vis_col1, vis_col2 = st.columns(2)
        
        with vis_col1:
            # Provider fraud distribution
            fraud_dist_path = DATA_DIR / "eda" / "provider_fraud_label_distribution.png"
            if fraud_dist_path.exists():
                st.image(str(fraud_dist_path), 
                        caption="Provider Fraud Label Distribution")
            else:
                st.warning("âŒ Provider fraud distribution plot not found")
                
            # Risk group analysis
            risk_group_path = DATA_DIR / "eda" / "eda_fraud_rate_by_riskgroup.png"
            if risk_group_path.exists():
                st.image(str(risk_group_path), 
                        caption="Fraud Rate by Provider Risk Group")
            else:
                st.warning("âŒ Risk group analysis plot not found")
        
        with vis_col2:
            # Total reimbursements boxplot
            reimb_boxplot_path = DATA_DIR / "eda" / "eda_total_reimbursements_boxplot.png"
            if reimb_boxplot_path.exists():
                st.image(str(reimb_boxplot_path), 
                        caption="Total Provider Reimbursement by Fraud Label")
            else:
                st.warning("âŒ Reimbursements boxplot not found")
                
            # Time series - claims over time
            claims_time_path = DATA_DIR / "eda" / "eda_claims_over_time.png"
            if claims_time_path.exists():
                st.image(str(claims_time_path), 
                        caption="Claims Over Time by Fraud Status")
            else:
                st.warning("âŒ Claims time series plot not found")
        
        # Additional time series visualization
        with st.expander("ğŸ“ˆ Additional Time Series Analysis"):
            reimb_time_path = DATA_DIR / "eda" / "eda_reimbursement_over_time.png"
            if reimb_time_path.exists():
                st.image(str(reimb_time_path), 
                        caption="Total Reimbursement Over Time by Provider Fraud Status", 
                        use_container_width=True)
            else:
                st.warning("âŒ Reimbursement time series plot not found")
                
        # Top outliers information - Enhanced table format
        if 'top_outliers' in data and data['top_outliers'] is not None:
            st.markdown("#### ğŸ” Top High-Risk Providers Analysis")
            
            # Create two columns for better layout
            table_col1, table_col2 = st.columns([2, 1])
            
            with table_col1:
                st.markdown("**Top 10 Providers by Total Reimbursement**")
                
                # Use the features dataset instead since top_outliers may have different columns
                if 'features' in data:
                    # Get top providers by total reimbursement from features dataset
                    top_providers = data['features'].nlargest(10, 'total_reimb')[['Provider', 'total_reimb', 'PotentialFraud', 'unique_beneficiaries']].copy()
                    
                    # Enhanced formatting
                    top_providers['Fraud_Status'] = top_providers['PotentialFraud'].map({
                        1: 'ğŸš¨ Fraudulent', 
                        0: 'âœ… Legitimate'
                    })
                    top_providers['Total_Reimbursements_Formatted'] = top_providers['total_reimb'].apply(lambda x: f"${x:,.0f}")
                    top_providers['Risk_Level'] = top_providers.apply(lambda row: 
                        'ğŸ”´ Very High' if row['PotentialFraud'] == 1 and row['total_reimb'] > 1000000 
                        else 'ğŸŸ¡ High' if row['total_reimb'] > 500000 
                        else 'ğŸŸ¢ Medium', axis=1)
                    
                    # Display enhanced table
                    display_cols = ['Provider', 'Total_Reimbursements_Formatted', 'unique_beneficiaries', 'Risk_Level', 'Fraud_Status']
                    display_table = top_providers[display_cols].copy()
                    display_table.columns = ['Provider ID', 'Total Reimbursement', 'Beneficiaries', 'Risk Level', 'Status']
                    
                    st.dataframe(
                        display_table,
                        use_container_width=True,
                        hide_index=True,
                        column_config={
                            "Provider ID": st.column_config.TextColumn("Provider ID", width="small"),
                            "Total Reimbursement": st.column_config.TextColumn("Total Reimbursement", width="medium"),
                            "Beneficiaries": st.column_config.NumberColumn("Beneficiaries", width="small"),
                            "Risk Level": st.column_config.TextColumn("Risk Level", width="small"),
                            "Status": st.column_config.TextColumn("Status", width="medium")
                        }
                    )
                else:
                    st.warning("âš ï¸ Provider analysis data not available")
            
            with table_col2:
                st.markdown("**ğŸ“Š Risk Distribution**")
                
                # Calculate risk statistics using the features dataset
                if 'features' in data:
                    top_providers_analysis = data['features'].nlargest(10, 'total_reimb')
                    
                    fraud_count = len(top_providers_analysis[top_providers_analysis['PotentialFraud'] == 1])
                    total_count = len(top_providers_analysis)
                    fraud_percentage = (fraud_count / total_count) * 100
                    
                    # Risk metrics
                    st.metric(
                        "Fraud Rate (Top 10)", 
                        f"{fraud_percentage:.0f}%",
                        delta=f"{fraud_count}/{total_count} providers"
                    )
                    
                    avg_fraud_in_top = top_providers_analysis[top_providers_analysis['PotentialFraud'] == 1]['total_reimb'].mean()
                    avg_legit_in_top = top_providers_analysis[top_providers_analysis['PotentialFraud'] == 0]['total_reimb'].mean()
                    
                    if not pd.isna(avg_fraud_in_top) and not pd.isna(avg_legit_in_top):
                        st.metric(
                            "Avg Fraud vs Legit", 
                            f"{avg_fraud_in_top/avg_legit_in_top:.1f}x",
                            delta="Higher reimbursement"
                        )
                    
                    # Total exposure from top providers
                    total_exposure = top_providers_analysis['total_reimb'].sum() / 1_000_000
                    st.metric(
                        "Total Exposure (Top 10)", 
                        f"${total_exposure:.1f}M",
                        delta="Combined reimbursement"
                    )
                else:
                    st.warning("âš ï¸ Risk analysis data not available")
        else:
            st.warning("âš ï¸ Top outliers data not available")
    else:
        st.warning("âš ï¸ EDA metrics not available - key insights cannot be displayed")
    
    # Key Project Insights
    st.subheader("ğŸ¯ Key Project Insights")
    
    # Enhanced project insights introduction
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 15px; color: white; margin-bottom: 1.5rem;">
    <h4 style="margin: 0; color: white;">ğŸš€ Project Configuration & Business Impact</h4>
    <p style="margin: 0.5rem 0;">Strategic model training approach and measurable business value delivered through clean, validated data</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Show model training split if available
    if data and 'counts_summary' in data:
        total_providers = data['counts_summary'].get('Providers in train', 5410)
        
        st.markdown("#### ğŸ² Model Training Configuration")
        col_train, col_val = st.columns(2)
        
        with col_train:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #36CFC9, #5CDBD3); padding: 1.5rem; border-radius: 15px; text-align: center; color: white;">
                <h3 style="margin: 0; font-size: 2rem;">{int(total_providers * 0.8):,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ‹ï¸ Training Set</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.9;">80% of providers</p>
            </div>
            """, unsafe_allow_html=True)
            
        with col_val:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #FAAD14, #FFC53D); padding: 1.5rem; border-radius: 15px; text-align: center; color: white;">
                <h3 style="margin: 0; font-size: 2rem;">{int(total_providers * 0.2):,}</h3>
                <p style="margin: 0; font-weight: bold;">ğŸ” Validation Set</p>
                <p style="margin: 0; font-size: 0.8rem; opacity: 0.9;">20% of providers</p>
            </div>
            """, unsafe_allow_html=True)
        
    # Business Value of Clean Data
    st.markdown("""
    <div class="success-card">
    <h4>ğŸ’¼ Business Value of Clean Data</h4>
    <ul>
      <li><strong>Reliable Insights:</strong> The cleaned dataset enables reliable fraud detection with zero missing critical fields.</li>
      <li><strong>Improved Decision-Making:</strong> Standardized formats (dates, encodings) provide consistent inputs for fraud detection algorithms.</li>
      <li><strong>System Integration:</strong> Clean, validated data ensures seamless integration with existing healthcare systems.</li>
      <li><strong>Audit-Ready:</strong> Complete data integrity checks maintain referential integrity between providers and claims.</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)

# Tab 2: Feature Analysis
with tab2:
    st.header("ğŸ—ï¸ Feature Analysis")
    
    # Feature Overview Section
    st.subheader("ğŸ“Š Feature Engineering Overview")
    
    # Load feature statistics and missing data
    feature_stats = None
    feature_missing = None
    
    feature_stats_path = DATA_DIR / "features_stats.csv"
    if feature_stats_path.exists():
        feature_stats = pd.read_csv(feature_stats_path, index_col=0)
    feature_missing_path = DATA_DIR / "features_missing.csv"
    if feature_missing_path.exists():
        feature_missing = pd.read_csv(feature_missing_path, index_col=0)
    
    if feature_stats is not None:
        total_features = len(feature_stats) - 1  # Exclude target variable
        
        # Feature overview metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h3 style="margin: 0; font-size: 2rem;">31</h3>
                <p style="margin: 0.5rem 0 0 0;">Total Features</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h3 style="margin: 0; font-size: 2rem;">5</h3>
                <p style="margin: 0.5rem 0 0 0;">Feature Categories</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            completeness = 99.7 if feature_missing is not None else 100.0
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h3 style="margin: 0; font-size: 2rem;">{completeness:.1f}%</h3>
                <p style="margin: 0.5rem 0 0 0;">Data Completeness</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h3 style="margin: 0; font-size: 2rem;">5,410</h3>
                <p style="margin: 0.5rem 0 0 0;">Total Providers</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Feature Categories Breakdown
        st.markdown("#### ğŸ“‹ Feature Categories")
        
        categories_data = {
            'Category': ['ğŸ“Š Claims Features', 'ğŸ’° Financial Features', 'ğŸ‘¥ Demographics Features', 'ï¿½ Medical Features', 'ğŸ“ˆ Behavioral Features'],
            'Count': ['4', '8', '5', '4', '10'],
            'Description': [
                'Total, inpatient, outpatient claims counts and ratios',
                'Reimbursement amounts, statistics, and deductibles', 
                'Patient age, mortality, gender, and race diversity',
                'Chronic conditions prevalence (Alzheimer, heart failure, etc.)',
                'Billing patterns, diagnosis diversity, time patterns'
            ],
            'Key Features': [
                'total_claims, inpatient_claims, outpatient_claims, inpt_outpt_ratio',
                'total_reimb, avg_reimb, median_reimb, std_reimb, max_reimb',
                'avg_age, pct_deceased, pct_male, race_diversity, unique_beneficiaries',
                'alzheimer_rate, heartfail_rate, kidney_rate, diabetes_rate',
                'claims_per_bene, avg_diag_diversity, pct_high_value, pct_weekend'
            ]
        }
        
        categories_df = pd.DataFrame(categories_data)
        st.dataframe(categories_df, use_container_width=True, hide_index=True,
                    column_config={
                        "Category": st.column_config.TextColumn("Feature Category", width="medium"),
                        "Count": st.column_config.TextColumn("Count", width="small"),
                        "Description": st.column_config.TextColumn("Description", width="large"),
                        "Key Features": st.column_config.TextColumn("Key Features", width="large")
                    })
    
    # Data Dictionary Section
    st.subheader("ğŸ“– Data Dictionary")
    
    # Comprehensive feature dictionary
    feature_dictionary = {
        'Feature': [
            'total_claims', 'inpatient_claims', 'outpatient_claims', 'inpt_outpt_ratio',
            'total_reimb', 'avg_reimb', 'median_reimb', 'std_reimb', 'max_reimb',
            'total_deductible', 'avg_deductible', 'claims_per_bene', 'unique_beneficiaries',
            'avg_age', 'pct_deceased', 'pct_male', 'race_diversity',
            'alzheimer_rate', 'heartfail_rate', 'kidney_rate', 'diabetes_rate',
            'pct_bene_multiclaim', 'avg_diag_diversity', 'avg_proc_diversity',
            'avg_days_between_claims', 'pct_high_value', 'pct_weekend',
            'pct_all_diag_filled', 'pct_all_proc_filled'
        ],
        'Category': [
            'Claims', 'Claims', 'Claims', 'Claims',
            'Financial', 'Financial', 'Financial', 'Financial', 'Financial',
            'Financial', 'Financial', 'Behavioral', 'Demographics',
            'Demographics', 'Demographics', 'Demographics', 'Demographics',
            'Medical', 'Medical', 'Medical', 'Medical',
            'Behavioral', 'Behavioral', 'Behavioral',
            'Behavioral', 'Behavioral', 'Behavioral',
            'Behavioral', 'Behavioral'
        ],
        'Description': [
            'Total number of claims submitted by provider',
            'Number of inpatient (hospital) claims',
            'Number of outpatient (ambulatory) claims', 
            'Ratio of inpatient to outpatient claims',
            'Total amount reimbursed to provider ($)',
            'Average reimbursement amount per claim ($)',
            'Median reimbursement amount per claim ($)',
            'Standard deviation of reimbursement amounts',
            'Maximum single claim reimbursement amount ($)',
            'Total deductible amounts paid ($)',
            'Average deductible amount per claim ($)',
            'Average number of claims per beneficiary',
            'Number of unique patients served',
            'Average age of patients served',
            'Percentage of patients who died',
            'Percentage of male patients',
            'Number of different racial groups served',
            'Percentage of patients with Alzheimer\'s',
            'Percentage of patients with heart failure',
            'Percentage of patients with kidney disease',
            'Percentage of patients with diabetes',
            'Percentage of patients with multiple claims',
            'Average number of different diagnoses per claim',
            'Average number of different procedures per claim',
            'Average days between consecutive claims',
            'Percentage of high-value claims (>$10,000)',
            'Percentage of claims submitted on weekends',
            'Percentage of claims with all diagnosis slots filled',
            'Percentage of claims with all procedure slots filled'
        ],
        'Business_Significance': [
            'High claim volume may indicate fraud or large practice',
            'Unusual inpatient ratios could signal fraud',
            'Primary care typically has more outpatient claims',
            'Extreme ratios may indicate billing manipulation',
            'Extremely high totals are fraud indicators',
            'Unusual average amounts may indicate upcoding',
            'Helps identify billing pattern anomalies',
            'High variation suggests inconsistent billing',
            'Outlier maximum amounts are red flags',
            'Unusual deductible patterns may indicate fraud',
            'Helps identify cost-shifting schemes',
            'Excessive claims per patient is suspicious',
            'Very few or too many patients is unusual',
            'Age patterns help identify specialty focus',
            'Mortality rates indicate patient severity',
            'Gender distribution affects risk profiles',
            'Diversity indicates broad patient base',
            'Chronic condition rates affect reimbursement',
            'Heart conditions increase legitimate costs',
            'Kidney disease affects billing patterns',
            'Diabetes prevalence impacts claim patterns',
            'Multiple claims may indicate care coordination',
            'Diagnosis complexity affects reimbursement',
            'Procedure diversity indicates specialization',
            'Rapid-fire claims may indicate fraud',
            'Excessive high-value claims are suspicious',
            'Weekend billing patterns may be unusual',
            'Over-documentation may indicate fraud',
            'Excessive procedure coding is suspicious'
        ]
    }
    
    dict_df = pd.DataFrame(feature_dictionary)
    
    # Make it searchable and interactive
    search_term = st.text_input("ğŸ” Search features (by name, category, or description):")
    
    if search_term:
        mask = (dict_df['Feature'].str.contains(search_term, case=False) |
                dict_df['Category'].str.contains(search_term, case=False) |
                dict_df['Description'].str.contains(search_term, case=False) |
                dict_df['Business_Significance'].str.contains(search_term, case=False))
        filtered_df = dict_df[mask]
    else:
        filtered_df = dict_df
    
    st.dataframe(filtered_df, use_container_width=True, hide_index=True,
                column_config={
                    "Feature": st.column_config.TextColumn("Feature Name", width="medium"),
                    "Category": st.column_config.TextColumn("Category", width="small"),
                    "Description": st.column_config.TextColumn("Description", width="large"),
                    "Business_Significance": st.column_config.TextColumn("Business Significance", width="large")
                })
    
    # Feature Statistics Section
    st.subheader("ğŸ“ˆ Feature Statistics")
    
    if feature_stats is not None:
        # Show basic stats
        st.markdown("#### Statistical Summary of All Features")
        
        # Format the statistics for better readability
        stats_display = feature_stats.copy()
        stats_display = stats_display.round(2)
        
        # Make it interactive
        selected_features = st.multiselect(
            "Select features to analyze (leave empty for all):",
            options=list(feature_stats.index),
            default=[]
        )
        
        if selected_features:
            stats_display = stats_display.loc[selected_features]
        
        st.dataframe(stats_display, use_container_width=True,
                    column_config={
                        "count": st.column_config.NumberColumn("Count"),
                        "mean": st.column_config.NumberColumn("Mean"),
                        "std": st.column_config.NumberColumn("Std Dev"),
                        "min": st.column_config.NumberColumn("Min"),
                        "25%": st.column_config.NumberColumn("25%"),
                        "50%": st.column_config.NumberColumn("Median"),
                        "75%": st.column_config.NumberColumn("75%"),
                        "max": st.column_config.NumberColumn("Max")
                    })
        
        # Key insights from statistics
        st.markdown("#### ğŸ’¡ Key Statistical Insights")
        
        insights_col1, insights_col2 = st.columns(2)
        
        with insights_col1:
            st.markdown("""
            <div style="background-color: #e8f4fd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
            <h5>ğŸ“Š Volume Insights</h5>
            <ul>
                <li><strong>Claims Distribution:</strong> Highly skewed (median: 31, max: 8,240)</li>
                <li><strong>Reimbursements:</strong> Extreme variation (median: $19,805, max: $5.9M)</li>
                <li><strong>Patient Load:</strong> Most providers serve 25-65 patients</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with insights_col2:
            st.markdown("""
            <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
            <h5>ğŸš¨ Fraud Indicators</h5>
            <ul>
                <li><strong>High-Value Claims:</strong> 4.5% average, some providers at 100%</li>
                <li><strong>Weekend Claims:</strong> 28.7% average, unusual patterns detected</li>
                <li><strong>Missing Claims Data:</strong> Only in avg_days_between_claims (32% missing)</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
    
    # Correlation Analysis Section
    st.subheader("ğŸ”— Correlation Analysis")
    
    # Display correlation heatmap
    col1, col2 = st.columns([2, 1])
    
    with col1:
        corr_heatmap_path = DATA_DIR / "eda" / "feature_corr_heatmap.png"
        if corr_heatmap_path.exists():
            st.image(str(corr_heatmap_path), 
                    caption="Feature Correlation Matrix", 
                    use_container_width=True)
        else:
            st.warning("âŒ Correlation heatmap not found")
    
    with col2:
        st.markdown("""
        <div style="background-color: #d4edda; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
        <h5>ğŸ¯ Correlation Insights</h5>
        <p><strong>Highly Correlated with Fraud:</strong></p>
        <ul>
            <li>total_reimb (0.79)</li>
            <li>total_deductible (0.32)</li>
            <li>max_reimb (0.25)</li>
            <li>unique_beneficiaries (0.24)</li>
            <li>inpatient_claims (0.19)</li>
        </ul>
        <p><strong>Key Finding:</strong> Financial features are strongest fraud predictors</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Top correlated features visualization
    st.markdown("#### ğŸ“Š Top Features Most Correlated with Fraud")
    
    corr_features_path = DATA_DIR / "eda" / "most_correlated_feature_with_fraud.png"
    if corr_features_path.exists():
        st.image(str(corr_features_path),
                caption="Boxplots of Top 5 Features Most Correlated with Fraud",
                use_container_width=True)
    else:
        st.warning("âŒ Top correlated features plot not found")
    
    # Feature Engineering Insights
    st.subheader("âš™ï¸ Feature Engineering Methodology")
    
    method_col1, method_col2 = st.columns(2)
    
    with method_col1:
        st.markdown("""
        <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #6c757d;">
        <h5>ğŸ”§ Engineering Process</h5>
        <ol>
            <li><strong>Claims Aggregation:</strong> Grouped claims by provider</li>
            <li><strong>Financial Metrics:</strong> Calculated reimbursement statistics</li>
            <li><strong>Patient Demographics:</strong> Aggregated beneficiary characteristics</li>
            <li><strong>Medical Patterns:</strong> Computed chronic condition rates</li>
            <li><strong>Behavioral Indicators:</strong> Derived billing pattern features</li>
        </ol>
        </div>
        """, unsafe_allow_html=True)
    
    with method_col2:
        st.markdown("""
        <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
        <h5>ğŸ’¼ Business Value</h5>
        <ul>
            <li><strong>Risk Stratification:</strong> Enables provider risk scoring</li>
            <li><strong>Pattern Detection:</strong> Identifies unusual billing behaviors</li>
            <li><strong>Cost Analysis:</strong> Quantifies financial impact</li>
            <li><strong>Clinical Context:</strong> Considers patient complexity</li>
            <li><strong>Audit Focus:</strong> Prioritizes investigation targets</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

# Tab 3: Model Performance
with tab3:
    st.header("ğŸ¤– Model Performance")
    
    # Executive Summary
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0;">
        <h3 style="margin: 0 0 1rem 0;">ğŸ¯ Executive Summary</h3>
        <p style="margin: 0; font-size: 1.1rem;"><strong>Selected Model:</strong> XGBoost Classifier</p>
        <p style="margin: 0.5rem 0 0 0;">Chosen for production deployment due to its superior fraud detection capability (76% recall), 
        ensuring maximum capture of fraudulent providers while maintaining acceptable precision (53%) for investigation workflows.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Load model performance metrics
    @st.cache_data
    def load_model_metrics():
        """Load all model performance metrics"""
        metrics = {}
        metrics_files = {
            'Logistic Regression': 'lr_metrics.json',
            'Decision Tree': 'dt_metrics.json', 
            'Random Forest': 'rf_gridsearch_metrics.json',
            'XGBoost': 'XGBost_metrics.json'
        }
        
        for model_name, filename in metrics_files.items():
            filepath = DATA_DIR / "metrics" / filename
            if filepath.exists():
                with open(filepath, 'r') as f:
                    metrics[model_name] = json.load(f)
            else:
                st.warning(f"âŒ Metrics file not found: {filepath}")
        
        return metrics
    
    model_metrics = load_model_metrics()
    
    if model_metrics:
        # Model Performance Overview
        st.subheader("ğŸ“Š Model Comparison Overview")
        
        # Create performance comparison table
        comparison_data = []
        for model_name, metrics in model_metrics.items():
            row = {
                'Model': model_name,
                'AUC-ROC': f"{metrics.get('auc', 0):.3f}",
                'Fraud Recall': f"{metrics.get('recall_fraud', 0):.3f}",
                'Fraud Precision': f"{metrics.get('precision_fraud', 0):.3f}",
                'Fraud F1-Score': f"{metrics.get('f1_fraud', 0):.3f}",
                'Accuracy': f"{metrics.get('accuracy', (metrics.get('recall_legit', 0) * 0.906 + metrics.get('recall_fraud', 0) * 0.094)):.3f}"
            }
            comparison_data.append(row)
        
        # Sort by AUC-ROC descending
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df['AUC_numeric'] = comparison_df['AUC-ROC'].astype(float)
        comparison_df = comparison_df.sort_values('AUC_numeric', ascending=False).drop('AUC_numeric', axis=1)
        
        # Style the dataframe to highlight best performances
        def highlight_best(s):
            """Highlight the best value in each column"""
            if s.name in ['AUC-ROC', 'Fraud Recall', 'Fraud Precision', 'Fraud F1-Score', 'Accuracy']:
                max_val = s.astype(float).max()
                return ['background-color: #d4edda' if float(v) == max_val else '' for v in s]
            return ['' for _ in s]
        
        styled_df = comparison_df.style.apply(highlight_best)
        st.dataframe(styled_df, use_container_width=True, hide_index=True)
        
        # Key Performance Insights
        st.markdown("#### ğŸ’¡ Key Performance Insights")
        
        # Find best performing models
        best_auc = comparison_df.loc[comparison_df['AUC-ROC'].astype(float).idxmax()]
        best_recall = comparison_df.loc[comparison_df['Fraud Recall'].astype(float).idxmax()]
        best_precision = comparison_df.loc[comparison_df['Fraud Precision'].astype(float).idxmax()]
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ† Production Model</h4>
                <h3 style="margin: 0.5rem 0;">XGBoost</h3>
                <p style="margin: 0;">Best for Fraud Detection</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ¯ Best Fraud Detection</h4>
                <h3 style="margin: 0.5rem 0;">{best_recall.iloc[0]}</h3>
                <p style="margin: 0;">Recall: {best_recall.iloc[2]}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ” Most Precise</h4>
                <h3 style="margin: 0.5rem 0;">{best_precision.iloc[0]}</h3>
                <p style="margin: 0;">Precision: {best_precision.iloc[3]}</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Detailed Model Analysis
        st.subheader("ğŸ” Detailed Model Analysis")
        
        # Model selection for detailed view
        model_list = list(model_metrics.keys())
        # Set XGBoost as default if available
        default_index = model_list.index('XGBoost') if 'XGBoost' in model_list else 0
        
        selected_model = st.selectbox(
            "Select a model for detailed analysis:",
            options=model_list,
            index=default_index
        )
        
        if selected_model and selected_model in model_metrics:
            metrics = model_metrics[selected_model]
            
            # Performance metrics cards
            st.markdown(f"#### {selected_model} - Performance Metrics")
            
            metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
            
            with metric_col1:
                st.metric(
                    label="AUC-ROC Score",
                    value=f"{metrics.get('auc', 0):.3f}",
                    delta=f"{(metrics.get('auc', 0) - 0.5):.3f} above random"
                )
            
            with metric_col2:
                st.metric(
                    label="Fraud Recall",
                    value=f"{metrics.get('recall_fraud', 0):.1%}",
                    delta="Higher is better"
                )
            
            with metric_col3:
                st.metric(
                    label="Fraud Precision", 
                    value=f"{metrics.get('precision_fraud', 0):.1%}",
                    delta="Higher is better"
                )
            
            with metric_col4:
                accuracy = metrics.get('accuracy', (metrics.get('recall_legit', 0) * 0.906 + metrics.get('recall_fraud', 0) * 0.094))
                st.metric(
                    label="Overall Accuracy",
                    value=f"{accuracy:.1%}",
                    delta="Balanced accuracy"
                )
            
            # Visualization Section
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("##### ğŸ“ˆ ROC Curve")
                roc_files = {
                    'Logistic Regression': 'roc_lr.png',
                    'Decision Tree': 'roc_dt.png',
                    'Random Forest': 'roc_rf_gridsearch.png',
                    'XGBoost': 'roc_XGBoost.png'
                }
                
                if selected_model in roc_files:
                    roc_path = DATA_DIR / "plots" / roc_files[selected_model]
                    if roc_path.exists():
                        st.image(str(roc_path), use_container_width=True)
                    else:
                        st.warning(f"ROC curve not found: {roc_path}")
            
            with col2:
                st.markdown("##### ğŸ”¢ Confusion Matrix")
                conf_files = {
                    'Logistic Regression': 'conf_matrix_lr.png',
                    'Decision Tree': 'conf_matrix_decision_tree.png', 
                    'Random Forest': 'conf_matrix_rf_gridsearch.png',
                    'XGBoost': 'conf_matrix_XGBoost.png'
                }
                
                if selected_model in conf_files:
                    conf_path = DATA_DIR / "plots" / conf_files[selected_model]
                    if conf_path.exists():
                        st.image(str(conf_path), use_container_width=True)
                    else:
                        st.warning(f"Confusion matrix not found: {conf_path}")
            
            # Business Impact Analysis
            st.markdown("##### ğŸ’¼ Business Impact Analysis")
            
            impact_col1, impact_col2 = st.columns(2)
            
            with impact_col1:
                st.markdown("""
                <div style="background-color: #e8f4fd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
                <h5>ğŸ“Š Performance Trade-offs</h5>
                """, unsafe_allow_html=True)
                
                fraud_recall = metrics.get('recall_fraud', 0)
                fraud_precision = metrics.get('precision_fraud', 0)
                
                # More nuanced performance analysis
                impact_text = ""
                if fraud_recall >= 0.76:
                    impact_text += "ğŸ¯ <strong>Excellent Fraud Detection:</strong> Catches 3 out of 4 fraud cases<br>"
                elif fraud_recall >= 0.6:
                    impact_text += "âœ… <strong>Good Fraud Detection:</strong> Catches majority of fraud cases<br>"
                elif fraud_recall >= 0.5:
                    impact_text += "âš ï¸ <strong>Moderate Fraud Detection:</strong> Catches about half of fraud cases<br>"
                else:
                    impact_text += "âŒ <strong>Low Fraud Detection:</strong> Misses many fraud cases<br>"
                
                if fraud_precision >= 0.8:
                    impact_text += "âœ… <strong>High Precision:</strong> Very few false alarms<br>"
                elif fraud_precision >= 0.6:
                    impact_text += "âœ… <strong>Good Precision:</strong> Manageable false alarm rate<br>"
                elif fraud_precision >= 0.4:
                    impact_text += "âš ï¸ <strong>Moderate Precision:</strong> Significant investigation load<br>"
                else:
                    impact_text += "âŒ <strong>Low Precision:</strong> High false alarm rate<br>"
                
                # Add model-specific insights
                if selected_model == "XGBoost":
                    impact_text += "<br><strong>XGBoost Trade-off:</strong> Optimized for fraud detection over precision. Accepts more false positives to minimize missed fraud cases."
                elif selected_model == "Random Forest":
                    impact_text += "<br><strong>Random Forest Trade-off:</strong> Balanced approach with good precision and recall."
                elif selected_model == "Logistic Regression":
                    impact_text += "<br><strong>Logistic Regression Trade-off:</strong> Strong baseline with interpretable results."
                else:
                    impact_text += "<br><strong>Note:</strong> Consider ensemble methods for better performance."
                
                st.markdown(impact_text + "</div>", unsafe_allow_html=True)
            
            with impact_col2:
                st.markdown("""
                <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
                <h5>ğŸ’° Financial Impact</h5>
                """, unsafe_allow_html=True)
                
                # Calculated financial impact based on real data
                total_providers = 5410
                fraud_rate = 0.094  # 9.4% fraud rate from data
                estimated_fraud_cases = int(total_providers * fraud_rate)
                detected_cases = int(estimated_fraud_cases * fraud_recall)
                
                # Average reimbursement for fraudulent providers (higher than legitimate)
                avg_fraud_reimbursement = 268736  # From features stats - higher end
                
                # False positives calculation
                legitimate_providers = int(total_providers * (1 - fraud_rate))
                false_positives = int(legitimate_providers * (1 - fraud_precision) * fraud_recall / fraud_precision) if fraud_precision > 0 else 0
                
                impact_text = f"""
                <p><strong>Fraud Detection:</strong> {detected_cases} out of {estimated_fraud_cases} fraud cases detected</p>
                <p><strong>Potential Savings:</strong> ${detected_cases * avg_fraud_reimbursement:,.0f}</p>
                <p><strong>Investigation Load:</strong> ~{false_positives} false positives requiring review</p>
                <p><strong>Detection Rate:</strong> {fraud_recall:.1%} of all fraud cases caught</p>
                """
                
                st.markdown(impact_text + "</div>", unsafe_allow_html=True)
        
        # Model Selection Recommendation
        st.subheader("ğŸ¯ Model Selection Recommendation")
        
        recommendation_col1, recommendation_col2 = st.columns(2)
        
        with recommendation_col1:
            st.markdown("""
            <div style="background-color: #d4edda; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
            <h5>ğŸ† Selected Production Model</h5>
            <p><strong>Chosen Model:</strong> XGBoost</p>
            <ul>
                <li><strong>Highest Fraud Recall (76%):</strong> Catches 3 out of 4 fraud cases</li>
                <li><strong>Strong AUC (0.947):</strong> Excellent discrimination ability</li>
                <li><strong>Business Priority:</strong> Maximizing fraud detection is critical</li>
                <li><strong>Cost-Benefit:</strong> Missing fraud is more expensive than false alarms</li>
            </ul>
            <p><strong>Why XGBoost:</strong> In healthcare fraud detection, the cost of missing a fraudulent provider far exceeds the cost of investigating false positives. XGBoost's superior recall ensures we catch the maximum number of fraud cases.</p>
            </div>
            """, unsafe_allow_html=True)
        
        with recommendation_col2:
            st.markdown("""
            <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #6c757d;">
            <h5>âš–ï¸ Alternative Option</h5>
            <p><strong>Secondary Model:</strong> Random Forest</p>
            <ul>
                <li><strong>Highest Precision (80%):</strong> Fewer false alarms</li>
                <li><strong>Best AUC (0.954):</strong> Superior discrimination</li>
                <li><strong>Stable Performance:</strong> More consistent results</li>
            </ul>
            <p><strong>Use Case:</strong> When investigation resources are limited and precision is critical</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Technical Implementation Notes
        st.subheader("âš™ï¸ Technical Implementation")
        
        # Create tabs for different technical aspects
        tech_tab1, tech_tab2, tech_tab3 = st.tabs(["ğŸ”§ XGBoost Details", "ğŸ“Š Training Process", "ğŸ¯ Model Selection"])
        
        with tech_tab1:
            st.markdown("""
            <div style="background-color: #e8f4fd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
            <h5>ï¿½ XGBoost Model Configuration</h5>
            <ul>
                <li><strong>Algorithm:</strong> Extreme Gradient Boosting (XGBoost)</li>
                <li><strong>Objective:</strong> Binary classification with logistic regression</li>
                <li><strong>Evaluation Metric:</strong> AUC-ROC</li>
                <li><strong>Class Imbalance Handling:</strong> scale_pos_weight = 9.686 (calculated from training data)</li>
                <li><strong>Hyperparameters (verified from notebook):</strong></li>
                <ul>
                    <li>n_estimators: 100</li>
                    <li>max_depth: 5</li>
                    <li>learning_rate: 0.1</li>
                    <li>random_state: 42</li>
                    <li>n_jobs: -1</li>
                    <li>use_label_encoder: False</li>
                    <li>eval_metric: 'auc'</li>
                </ul>
                <li><strong>Training Features:</strong> 28 engineered provider-level features</li>
                <li><strong>Validation:</strong> eval_set monitoring on validation data (verbose=False)</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with tech_tab2:
            st.markdown("""
            <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #6c757d;">
            <h5>ğŸ“ˆ Training Pipeline</h5>
            <ul>
                <li><strong>Data Split:</strong> 80/20 stratified train/validation split (random_state=42)</li>
                <li><strong>Training Set:</strong> 4,328 providers (91% legitimate, 9% fraud)</li>
                <li><strong>Validation Set:</strong> 1,082 providers (same class distribution)</li>
                <li><strong>API Used:</strong> scikit-learn XGBoost API (not PySpark)</li>
                <li><strong>Feature Engineering:</strong> Provider-level aggregations from claims data (28 features)</li>
                <li><strong>Preprocessing:</strong> Missing value imputation, no scaling required for XGBoost</li>
                <li><strong>Class Balancing:</strong> scale_pos_weight automatic calculation</li>
                <li><strong>Training Method:</strong> clf.fit(X_train, y_train, eval_set=[(X_val, y_val)])</li>
                <li><strong>Monitoring:</strong> AUC-ROC tracked on validation set (verbose=False)</li>
                <li><strong>Model Persistence:</strong> Saved in pickle (.pkl) and native XGBoost (.json) formats</li>
                <li><strong>Experiment Tracking:</strong> MLflow logging with run name "XGBoost Classifier"</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with tech_tab3:
            st.markdown("""
            <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
            <h5>ğŸ¯ Model Selection Methodology</h5>
            <p><strong>Evaluation Framework:</strong></p>
            <ul>
                <li><strong>Primary Metric:</strong> Fraud Recall (Sensitivity) - Critical for catching fraud</li>
                <li><strong>Secondary Metric:</strong> AUC-ROC - Overall discriminative ability</li>
                <li><strong>Business Constraint:</strong> Precision must be â‰¥50% for operational feasibility</li>
                <li><strong>Validation:</strong> Stratified hold-out validation (no k-fold due to time constraints)</li>
            </ul>
            <p><strong>Why XGBoost Won:</strong></p>
            <ul>
                <li><strong>Highest Fraud Recall (76%):</strong> Catches 385 out of 508 fraud cases</li>
                <li><strong>Built-in Class Imbalance Handling:</strong> scale_pos_weight parameter</li>
                <li><strong>Robust to Feature Outliers:</strong> Tree-based algorithm handles extreme values</li>
                <li><strong>Feature Importance Available:</strong> Supports model interpretability requirements</li>
                <li><strong>Fast Inference:</strong> Suitable for real-time scoring of new providers</li>
                <li><strong>Industry Standard:</strong> Proven performance in financial fraud detection</li>
            </ul>
            <p><strong>Real-World Deployment Considerations:</strong></p>
            <ul>
                <li><strong>Model Drift Monitoring:</strong> Track feature distributions and performance decay</li>
                <li><strong>Threshold Optimization:</strong> Adjust decision threshold based on investigation capacity</li>
                <li><strong>Retraining Schedule:</strong> Quarterly retraining with new claims data</li>
                <li><strong>A/B Testing:</strong> Compare against current rule-based system</li>
                <li><strong>Regulatory Compliance:</strong> Document model decisions for audit trails</li>
                <li><strong>Human-in-the-Loop:</strong> Model flags for investigator review, not automatic action</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
    else:
        st.error("âŒ No model performance metrics found. Please ensure the metrics files are available.")

with tab4:
    st.header("ğŸ“Š SHAP Explainability")
    
    # Executive Summary
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0;">
        <h3 style="margin: 0 0 1rem 0;">ğŸ¯ Model Transparency & Explainability</h3>
        <p style="margin: 0; font-size: 1.1rem;">Understanding <strong>why</strong> the XGBoost model flags providers as fraudulent using SHAP (SHapley Additive exPlanations) analysis.</p>
        <p style="margin: 0.5rem 0 0 0;">SHAP provides both global insights (feature importance across all providers) and local explanations (why specific providers were flagged).</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Load SHAP data
    @st.cache_data
    def load_shap_data():
        """Load SHAP analysis data and supporting datasets"""
        shap_data = {}
        
        try:
            # Load feature importance
            shap_importance_path = DATA_DIR / "validation_sets" / "shap_feature_importance.csv"
            if shap_importance_path.exists():
                shap_data['feature_importance'] = pd.read_csv(shap_importance_path)
            
            # Load validation features for provider analysis
            X_val_path = DATA_DIR / "validation_sets" / "X_val_full.parquet"
            if X_val_path.exists():
                shap_data['X_val'] = pd.read_parquet(X_val_path)
            
            # Load predictions
            pred_path = DATA_DIR / "validation_sets" / "xgb_val_proba.csv"
            if pred_path.exists():
                shap_data['predictions'] = pd.read_csv(pred_path)
            
            # Load true labels
            y_val_path = DATA_DIR / "validation_sets" / "y_val.csv"
            if y_val_path.exists():
                shap_data['y_val'] = pd.read_csv(y_val_path)
            
            # Load SHAP values if available
            shap_values_path = DATA_DIR / "validation_sets" / "shap_values.npy"
            if shap_values_path.exists():
                shap_data['shap_values'] = np.load(shap_values_path)
            
            # Load example provider data
            provider13_path = DATA_DIR / "validation_sets" / "provider13_features.csv"
            if provider13_path.exists():
                shap_data['example_provider'] = pd.read_csv(provider13_path)
                
        except Exception as e:
            st.error(f"Error loading SHAP data: {e}")
        
        return shap_data
    
    shap_data = load_shap_data()
    
    if not shap_data:
        st.error("âŒ SHAP analysis data not found. Please ensure SHAP analysis has been completed.")
        st.stop()
    
    # Key Insights Summary
    st.subheader("ğŸ’¡ Key Fraud Detection Insights")
    
    if 'feature_importance' in shap_data and not shap_data['feature_importance'].empty:
        feature_importance_df = shap_data['feature_importance']
        
        # Check if the expected columns exist
        if 'feature' in feature_importance_df.columns and 'mean_abs_shap' in feature_importance_df.columns:
            top_features = feature_importance_df.head(3)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                feature_1 = top_features.iloc[0]
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                    <h4 style="margin: 0;">ğŸš¨ #1 Fraud Indicator</h4>
                    <h3 style="margin: 0.5rem 0;">{feature_1['feature'].replace('_', ' ').title()}</h3>
                    <p style="margin: 0;">SHAP Impact: {feature_1['mean_abs_shap']:.3f}</p>
                    <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Total reimbursement amount is the strongest predictor of fraudulent activity</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                feature_2 = top_features.iloc[1]
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #ffa726 0%, #fb8c00 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                    <h4 style="margin: 0;">âš ï¸ #2 Risk Factor</h4>
                    <h3 style="margin: 0.5rem 0;">{feature_2['feature'].replace('_', ' ').title()}</h3>
                    <p style="margin: 0;">SHAP Impact: {feature_2['mean_abs_shap']:.3f}</p>
                    <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Patient population with kidney disease affects fraud risk</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col3:
                feature_3 = top_features.iloc[2]
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #42a5f5 0%, #1e88e5 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                    <h4 style="margin: 0;">ğŸ“Š #3 Pattern Indicator</h4>
                    <h3 style="margin: 0.5rem 0;">{feature_3['feature'].replace('_', ' ').title()}</h3>
                    <p style="margin: 0;">SHAP Impact: {feature_3['mean_abs_shap']:.3f}</p>
                    <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Heart failure patient rates signal billing patterns</p>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.warning("âš ï¸ SHAP feature importance data format is incompatible. Expected columns: 'feature' and 'mean_abs_shap'")
            if not feature_importance_df.empty:
                st.info(f"Available columns: {list(feature_importance_df.columns)}")
    else:
        st.warning("âš ï¸ SHAP feature importance data not available - using fallback insights")
        
        # Fallback insights when SHAP data is not available
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸš¨ #1 Fraud Indicator</h4>
                <h3 style="margin: 0.5rem 0;">Total Reimbursement</h3>
                <p style="margin: 0;">Key fraud predictor</p>
                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Higher reimbursements strongly correlate with fraud</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #ffa726 0%, #fb8c00 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">âš ï¸ #2 Risk Factor</h4>
                <h3 style="margin: 0.5rem 0;">Claims Volume</h3>
                <p style="margin: 0;">Billing pattern indicator</p>
                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Unusual claim frequencies signal potential fraud</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("""
            <div style="background: linear-gradient(135deg, #42a5f5 0%, #1e88e5 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ“Š #3 Pattern Indicator</h4>
                <h3 style="margin: 0.5rem 0;">Patient Demographics</h3>
                <p style="margin: 0;">Risk profile marker</p>
                <p style="margin: 0.5rem 0 0 0; font-size: 0.9rem;">Patient population characteristics affect risk assessment</p>
            </div>
            """, unsafe_allow_html=True)
    
    # Global Feature Importance Analysis
    st.subheader("ğŸŒ Global Feature Importance")
    
    if 'feature_importance' in shap_data:
        st.markdown("#### ğŸ“ˆ SHAP Feature Importance Ranking")
        
        # Display top features in an interactive way
        feature_df = shap_data['feature_importance'].head(15).copy()
        feature_df['Business Category'] = feature_df['feature'].map({
            'total_reimb': 'ğŸ’° Financial',
            'kidney_rate': 'ğŸ¥ Medical',
            'heartfail_rate': 'ğŸ¥ Medical', 
            'claims_per_bene': 'ğŸ“Š Behavioral',
            'median_reimb': 'ğŸ’° Financial',
            'avg_days_between_claims': 'ğŸ“Š Behavioral',
            'pct_bene_multiclaim': 'ğŸ“Š Behavioral',
            'avg_diag_diversity': 'ğŸ¥ Medical',
            'pct_weekend': 'ğŸ“Š Behavioral',
            'pct_deceased': 'ğŸ‘¥ Demographics',
            'avg_deductible': 'ğŸ’° Financial',
            'pct_male': 'ğŸ‘¥ Demographics',
            'max_reimb': 'ğŸ’° Financial',
            'alzheimer_rate': 'ğŸ¥ Medical',
            'avg_age': 'ğŸ‘¥ Demographics'
        })
        
        feature_df['Feature Name'] = feature_df['feature'].str.replace('_', ' ').str.title()
        feature_df['SHAP Importance'] = feature_df['mean_abs_shap'].round(3)
        
        # Create business interpretations
        business_interpretations = {
            'total_reimb': 'Providers with unusually high total reimbursements are strong fraud indicators',
            'kidney_rate': 'High rates of kidney disease patients may indicate patient mix manipulation',
            'heartfail_rate': 'Heart failure patient concentration affects billing complexity and risk',
            'claims_per_bene': 'Excessive claims per beneficiary suggests over-utilization',
            'median_reimb': 'Median reimbursement patterns reveal billing consistency',
            'avg_days_between_claims': 'Rapid-fire claim submissions may indicate systematic fraud',
            'pct_bene_multiclaim': 'Multiple claims per patient can indicate care coordination or fraud',
            'avg_diag_diversity': 'Diagnosis diversity reflects provider specialization and billing complexity',
            'pct_weekend': 'Weekend billing patterns may indicate unusual operational practices',
            'pct_deceased': 'Patient mortality rates affect claim patterns and provider risk profiles'
        }
        
        feature_df['Business Interpretation'] = feature_df['feature'].map(business_interpretations).fillna('Contributes to overall fraud risk assessment')
        
        # Display the enhanced feature importance table
        display_df = feature_df[['Feature Name', 'Business Category', 'SHAP Importance', 'Business Interpretation']].copy()
        
        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "Feature Name": st.column_config.TextColumn("Feature", width="medium"),
                "Business Category": st.column_config.TextColumn("Category", width="small"),
                "SHAP Importance": st.column_config.NumberColumn("SHAP Impact", width="small"),
                "Business Interpretation": st.column_config.TextColumn("Business Meaning", width="large")
            }
        )
        
        # Feature Category Analysis
        st.markdown("#### ğŸ“‹ Feature Impact by Category")
        
        if 'Business Category' in feature_df.columns:
            category_impact = feature_df.groupby('Business Category')['mean_abs_shap'].agg(['sum', 'count', 'mean']).round(3)
            category_impact.columns = ['Total Impact', 'Feature Count', 'Avg Impact']
            category_impact = category_impact.sort_values('Total Impact', ascending=False)
            
            cat_col1, cat_col2 = st.columns(2)
            
            with cat_col1:
                st.markdown("**Category Impact Summary:**")
                st.dataframe(category_impact, use_container_width=True)
            
            with cat_col2:
                st.markdown("**Key Insights:**")
                st.markdown("""
                - **ğŸ’° Financial features** dominate fraud detection
                - **ğŸ¥ Medical patterns** provide important context  
                - **ğŸ“Š Behavioral indicators** reveal operational anomalies
                - **ğŸ‘¥ Demographics** offer supporting evidence
                """)
    
    # Visualizations
    st.subheader("ğŸ“Š SHAP Visualizations")
    
    viz_col1, viz_col2 = st.columns(2)
    
    with viz_col1:
        st.markdown("##### ğŸ“ˆ Global Feature Importance")
        shap_bar_path = DATA_DIR / "plots" / "shap_summary_bar.png"
        if shap_bar_path.exists():
            st.image(str(shap_bar_path), 
                    caption="SHAP Summary Bar Plot - Global feature importance ranking",
                    use_container_width=True)
        else:
            st.warning("ğŸ“Š SHAP summary bar plot not available")
    
    with viz_col2:
        st.markdown("##### ğŸ¯ Feature Impact Distribution")
        shap_beeswarm_path = DATA_DIR / "plots" / "shap_beeswarm.png"
        if shap_beeswarm_path.exists():
            st.image(str(shap_beeswarm_path),
                    caption="SHAP Beeswarm Plot - Feature value impact on predictions",
                    use_container_width=True)
        else:
            st.warning("ğŸ“Š SHAP beeswarm plot not available")
    
    # Individual Provider Analysis
    st.subheader("ğŸ” Individual Provider Analysis")
    
    # Provider selection and analysis
    if 'X_val' in shap_data and 'predictions' in shap_data:
        
        st.markdown("#### ğŸ¯ Sample High-Risk Provider Analysis")
        
        # Show the example provider (Provider 13 from the notebook)
        if 'example_provider' in shap_data:
            example_data = shap_data['example_provider']
            
            # Provider overview
            provider_col1, provider_col2, provider_col3 = st.columns(3)
            
            with provider_col1:
                st.metric("Total Claims", f"{int(example_data.iloc[0]['total_claims'])}")
                st.metric("Total Reimbursement", f"${int(example_data.iloc[0]['total_reimb']):,}")
            
            with provider_col2:
                st.metric("Inpatient Claims", f"{int(example_data.iloc[0]['inpatient_claims'])}")
                st.metric("Average Reimbursement", f"${example_data.iloc[0]['avg_reimb']:,.0f}")
            
            with provider_col3:
                st.metric("Unique Patients", f"{int(example_data.iloc[0]['unique_beneficiaries'])}")
                inpt_ratio = example_data.iloc[0]['inpt_outpt_ratio']
                st.metric("Inpatient/Outpatient Ratio", f"{inpt_ratio:.2f}")
            
            # Force plot visualization
            st.markdown("##### ğŸ­ SHAP Force Plot - Why This Provider Was Flagged")
            
            shap_force_path = DATA_DIR / "plots" / "shap_force_provider_13.png"
            if shap_force_path.exists():
                st.image(str(shap_force_path),
                        caption="SHAP Force Plot showing feature contributions to fraud prediction",
                        use_container_width=True)
            else:
                st.warning("ğŸ“Š Provider force plot not available")
            
            # Key risk factors for this provider
            st.markdown("##### ğŸš¨ Key Risk Factors for This Provider")
            
            risk_col1, risk_col2 = st.columns(2)
            
            with risk_col1:
                st.markdown("""
                <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
                <h5>âš ï¸ Primary Risk Indicators</h5>
                <ul>
                    <li><strong>Extremely High Reimbursement:</strong> $811,460 total (far above average)</li>
                    <li><strong>High Inpatient Volume:</strong> 97 inpatient claims (unusual pattern)</li>
                    <li><strong>Skewed Claim Ratio:</strong> 3.03 inpatient/outpatient ratio</li>
                    <li><strong>High Average Billing:</strong> $6,290 per claim</li>
                </ul>
                </div>
                """, unsafe_allow_html=True)
            
            with risk_col2:
                st.markdown("""
                <div style="background-color: #d4edda; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
                <h5>ğŸ” Investigation Focus Areas</h5>
                <ul>
                    <li><strong>Billing Verification:</strong> Validate high-value claims</li>
                    <li><strong>Patient Records:</strong> Review inpatient admission criteria</li>
                    <li><strong>Procedure Coding:</strong> Check for upcoding patterns</li>
                    <li><strong>Medical Necessity:</strong> Audit clinical documentation</li>
                </ul>
                </div>
                """, unsafe_allow_html=True)
    
    # Business Intelligence Dashboard
    st.subheader("ğŸ’¼ Business Intelligence & Audit Guidance")
    
    intel_col1, intel_col2 = st.columns(2)
    
    with intel_col1:
        st.markdown("""
        <div style="background-color: #e8f4fd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
        <h5>ğŸ¯ Audit Prioritization Strategy</h5>
        <ol>
            <li><strong>High Total Reimbursement:</strong> Focus on providers in top 5% of reimbursements</li>
            <li><strong>Medical Pattern Anomalies:</strong> Investigate unusual chronic disease rates</li>
            <li><strong>Behavioral Red Flags:</strong> Examine rapid claim submission patterns</li>
            <li><strong>Combined Risk Factors:</strong> Prioritize providers with multiple indicators</li>
        </ol>
        <p><strong>ROI Focus:</strong> SHAP analysis shows financial features have highest predictive power</p>
        </div>
        """, unsafe_allow_html=True)
    
    with intel_col2:
        st.markdown("""
        <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #6c757d;">
        <h5>ğŸ“‹ Compliance & Documentation</h5>
        <ul>
            <li><strong>Explainable Decisions:</strong> SHAP provides audit-ready explanations</li>
            <li><strong>Regulatory Support:</strong> Transparent AI for compliance review</li>
            <li><strong>Appeal Defense:</strong> Feature-based justifications for investigations</li>
            <li><strong>Continuous Monitoring:</strong> Track SHAP patterns over time</li>
        </ul>
        <p><strong>Documentation:</strong> Every fraud flag backed by quantified feature contributions</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Model Transparency Statement
    st.subheader("ğŸ” Model Transparency & Limitations")
    
    st.markdown("""
    <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
    <h5>âš–ï¸ Ethical AI & Fairness Considerations</h5>
    <ul>
        <li><strong>Human Oversight Required:</strong> Model predictions are recommendations, not final decisions</li>
        <li><strong>Regular Bias Monitoring:</strong> SHAP values tracked for demographic fairness</li>
        <li><strong>Appeal Process:</strong> Providers can challenge flags with additional evidence</li>
        <li><strong>Continuous Improvement:</strong> Model updated quarterly with new data and patterns</li>
    </ul>
    <p><strong>Important:</strong> This tool supports investigators - human judgment remains essential for final fraud determinations.</p>
    </div>
    """, unsafe_allow_html=True)

with tab5:
    st.header("ğŸ•µï¸ Fraud Risk Predictor")
    
    # Load model and data
    @st.cache_resource
    def load_fraud_model():
        """Load the trained XGBoost model"""
        try:
            import pickle
            with open(str(DATA_DIR / "models" / "xgb_classifier.pkl"), 'rb') as f:
                model = pickle.load(f)
            return model
        except Exception as e:
            st.error(f"âŒ Model not available: {e}")
            return None
    
    @st.cache_data
    def get_feature_ranges():
        """Get realistic ranges for sliders from feature stats"""
        try:
            stats = pd.read_csv(str(DATA_DIR / "features_stats.csv"), index_col=0)
            ranges = {}
            for feature in stats.index:
                if feature != 'PotentialFraud':  # Exclude target variable
                    ranges[feature] = {
                        'min': float(stats.loc[feature, 'min']),
                        'max': min(float(stats.loc[feature, '75%']) * 2, float(stats.loc[feature, 'max'])),
                        'default': float(stats.loc[feature, '50%']),
                        'q25': float(stats.loc[feature, '25%']),
                        'q75': float(stats.loc[feature, '75%'])
                    }
            return ranges
        except Exception as e:
            st.error(f"âŒ Feature ranges not available: {e}")
            return {}
    
    @st.cache_data
    def get_sample_providers():
        """Load sample providers for randomization"""
        try:
            return pd.read_parquet('data/validation_sets/X_val_full.parquet')
        except:
            return None
    
    # Feature definitions with business-friendly names
    feature_definitions = {
        'total_claims': {
            'name': 'Total Claims Submitted',
            'help': 'Total number of claims submitted by this provider',
            'format': 'int',
            'category': 'Claims Volume'
        },
        'inpatient_claims': {
            'name': 'Inpatient Claims',
            'help': 'Number of hospital/inpatient claims',
            'format': 'int',
            'category': 'Claims Volume'
        },
        'outpatient_claims': {
            'name': 'Outpatient Claims', 
            'help': 'Number of outpatient/ambulatory claims',
            'format': 'int',
            'category': 'Claims Volume'
        },
        'total_reimb': {
            'name': 'Total Reimbursement ($)',
            'help': 'Total amount reimbursed to provider',
            'format': 'currency',
            'category': 'Financial'
        },
        'avg_reimb': {
            'name': 'Average Claim Amount ($)',
            'help': 'Average reimbursement per claim',
            'format': 'currency',
            'category': 'Financial'
        },
        'max_reimb': {
            'name': 'Highest Single Claim ($)',
            'help': 'Maximum amount for a single claim',
            'format': 'currency',
            'category': 'Financial'
        },
        'total_deductible': {
            'name': 'Total Deductibles ($)',
            'help': 'Total deductible amounts collected',
            'format': 'currency',
            'category': 'Financial'
        },
        'unique_beneficiaries': {
            'name': 'Number of Patients',
            'help': 'Number of unique patients served',
            'format': 'int',
            'category': 'Patient Demographics'
        },
        'avg_age': {
            'name': 'Average Patient Age',
            'help': 'Average age of patients served',
            'format': 'float',
            'category': 'Patient Demographics'
        },
        'pct_male': {
            'name': 'Male Patient Rate (%)',
            'help': 'Percentage of male patients',
            'format': 'percent',
            'category': 'Patient Demographics'
        },
        'alzheimer_rate': {
            'name': 'Alzheimer\'s Patient Rate (%)',
            'help': 'Percentage of patients with Alzheimer\'s disease',
            'format': 'percent',
            'category': 'Medical Complexity'
        },
        'heartfail_rate': {
            'name': 'Heart Failure Patient Rate (%)',
            'help': 'Percentage of patients with heart failure',
            'format': 'percent',
            'category': 'Medical Complexity'
        },
        'kidney_rate': {
            'name': 'Kidney Disease Patient Rate (%)',
            'help': 'Percentage of patients with kidney disease',
            'format': 'percent',
            'category': 'Medical Complexity'
        },
        'diabetes_rate': {
            'name': 'Diabetes Patient Rate (%)',
            'help': 'Percentage of patients with diabetes',
            'format': 'percent',
            'category': 'Medical Complexity'
        },
        'claims_per_bene': {
            'name': 'Claims per Patient',
            'help': 'Average number of claims per patient',
            'format': 'float',
            'category': 'Billing Patterns'
        },
        'pct_weekend': {
            'name': 'Weekend Claims Rate (%)',
            'help': 'Percentage of claims submitted on weekends',
            'format': 'percent',
            'category': 'Billing Patterns'
        },
        'avg_diag_diversity': {
            'name': 'Diagnosis Diversity',
            'help': 'Average number of different diagnoses per claim',
            'format': 'float',
            'category': 'Billing Patterns'
        }
    }
    
    # Load model and data
    model = load_fraud_model()
    ranges = get_feature_ranges()
    sample_data = get_sample_providers()
    
    if model is None or not ranges:
        st.error("âŒ Required model or data files not available. Please ensure all files are properly loaded.")
        st.stop()
    
    # Main interface
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0;">
        <h3 style="margin: 0 0 1rem 0;">ğŸ›ï¸ Interactive Provider Risk Assessment</h3>
        <p style="margin: 0; font-size: 1.1rem;">Adjust provider characteristics below to see real-time fraud risk predictions from our XGBoost model.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Control buttons
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸ² Randomize Provider", use_container_width=True):
            st.session_state.randomize = True
    
    with col2:
        if st.button("ğŸ”„ Reset to Defaults", use_container_width=True):
            st.session_state.reset = True
    
    with col3:
        if st.button("ğŸ“Š Load High Risk Example", use_container_width=True):
            st.session_state.load_high_risk = True
    
    with col4:
        if st.button("âœ… Load Low Risk Example", use_container_width=True):
            st.session_state.load_low_risk = True
    
    # Initialize session state for features
    if 'provider_features' not in st.session_state:
        st.session_state.provider_features = {}
        for feature in feature_definitions.keys():
            if feature in ranges:
                st.session_state.provider_features[feature] = ranges[feature]['default']
    
    # Handle button actions
    if st.session_state.get('randomize', False):
        if sample_data is not None:
            random_provider = sample_data.sample(1).iloc[0]
            for feature in feature_definitions.keys():
                if feature in random_provider:
                    st.session_state.provider_features[feature] = float(random_provider[feature])
        st.session_state.randomize = False
        st.rerun()
    
    if st.session_state.get('reset', False):
        for feature in feature_definitions.keys():
            if feature in ranges:
                st.session_state.provider_features[feature] = ranges[feature]['default']
        st.session_state.reset = False
        st.rerun()
    
    if st.session_state.get('load_high_risk', False):
        # High risk profile - high financial values, high chronic disease rates
        high_risk_profile = {
            'total_claims': 250,
            'inpatient_claims': 180,
            'outpatient_claims': 70,
            'total_reimb': 450000,
            'avg_reimb': 1800,
            'max_reimb': 25000,
            'total_deductible': 50000,
            'unique_beneficiaries': 80,
            'avg_age': 78,
            'pct_male': 0.45,
            'alzheimer_rate': 0.85,
            'heartfail_rate': 0.90,
            'kidney_rate': 0.80,
            'diabetes_rate': 0.95,
            'claims_per_bene': 3.1,
            'pct_weekend': 0.45,
            'avg_diag_diversity': 8.5
        }
        for feature, value in high_risk_profile.items():
            if feature in st.session_state.provider_features:
                st.session_state.provider_features[feature] = value
        st.session_state.load_high_risk = False
        st.rerun()
    
    if st.session_state.get('load_low_risk', False):
        # Low risk profile - typical values
        low_risk_profile = {
            'total_claims': 25,
            'inpatient_claims': 2,
            'outpatient_claims': 23,
            'total_reimb': 15000,
            'avg_reimb': 600,
            'max_reimb': 2500,
            'total_deductible': 800,
            'unique_beneficiaries': 20,
            'avg_age': 72,
            'pct_male': 0.42,
            'alzheimer_rate': 0.35,
            'heartfail_rate': 0.55,
            'kidney_rate': 0.38,
            'diabetes_rate': 0.68,
            'claims_per_bene': 1.25,
            'pct_weekend': 0.25,
            'avg_diag_diversity': 2.8
        }
        for feature, value in low_risk_profile.items():
            if feature in st.session_state.provider_features:
                st.session_state.provider_features[feature] = value
        st.session_state.load_low_risk = False
        st.rerun()
    
    # Feature input controls organized by category
    st.subheader("ğŸ›ï¸ Provider Characteristics")
    
    # Group features by category
    categories = {}
    for feature, definition in feature_definitions.items():
        category = definition['category']
        if category not in categories:
            categories[category] = []
        categories[category].append((feature, definition))
    
    # Create feature inputs by category
    for category, features in categories.items():
        with st.expander(f"ğŸ“Š {category}", expanded=True):
            cols = st.columns(2)
            for i, (feature, definition) in enumerate(features):
                if feature in ranges:
                    with cols[i % 2]:
                        if definition['format'] == 'percent':
                            value = st.slider(
                                definition['name'],
                                min_value=0.0,
                                max_value=1.0,
                                value=float(st.session_state.provider_features.get(feature, ranges[feature]['default'])),
                                step=0.01,
                                help=definition['help'],
                                key=f"slider_{feature}"
                            )
                            st.caption(f"Current value: {value:.1%}")
                        elif definition['format'] == 'currency':
                            value = st.slider(
                                definition['name'],
                                min_value=float(ranges[feature]['min']),
                                max_value=float(ranges[feature]['max']),
                                value=float(st.session_state.provider_features.get(feature, ranges[feature]['default'])),
                                step=max(1.0, (ranges[feature]['max'] - ranges[feature]['min']) / 1000),
                                help=definition['help'],
                                key=f"slider_{feature}"
                            )
                            st.caption(f"Current value: ${value:,.0f}")
                        elif definition['format'] == 'int':
                            value = st.slider(
                                definition['name'],
                                min_value=int(ranges[feature]['min']),
                                max_value=int(ranges[feature]['max']),
                                value=int(st.session_state.provider_features.get(feature, ranges[feature]['default'])),
                                step=1,
                                help=definition['help'],
                                key=f"slider_{feature}"
                            )
                        else:  # float
                            value = st.slider(
                                definition['name'],
                                min_value=float(ranges[feature]['min']),
                                max_value=float(ranges[feature]['max']),
                                value=float(st.session_state.provider_features.get(feature, ranges[feature]['default'])),
                                step=float((ranges[feature]['max'] - ranges[feature]['min']) / 100),
                                help=definition['help'],
                                key=f"slider_{feature}"
                            )
                            st.caption(f"Current value: {value:.2f}")
                        
                        st.session_state.provider_features[feature] = value
    
    # Prepare features for prediction
    feature_values = []
    feature_names = []
    
    # Get all 28 features in the correct order (matching training data)
    all_features = [
        'total_claims', 'inpatient_claims', 'outpatient_claims', 'total_reimb', 'avg_reimb',
        'median_reimb', 'std_reimb', 'max_reimb', 'total_deductible', 'avg_deductible',
        'inpt_outpt_ratio', 'claims_per_bene', 'unique_beneficiaries', 'avg_age',
        'pct_deceased', 'pct_male', 'race_diversity', 'alzheimer_rate', 'heartfail_rate',
        'kidney_rate', 'diabetes_rate', 'pct_bene_multiclaim', 'avg_diag_diversity',
        'avg_proc_diversity', 'avg_days_between_claims', 'pct_high_value', 'pct_weekend',
        'pct_all_diag_filled'
    ]
    
    # Fill in feature values (use defaults for non-interactive features)
    for feature in all_features:
        if feature in st.session_state.provider_features:
            feature_values.append(st.session_state.provider_features[feature])
        elif feature in ranges:
            # Auto-calculate derived features
            if feature == 'inpt_outpt_ratio':
                inpt = st.session_state.provider_features.get('inpatient_claims', 0)
                outpt = st.session_state.provider_features.get('outpatient_claims', 1)
                ratio = inpt / max(outpt, 1)
                feature_values.append(ratio)
            elif feature == 'median_reimb':
                feature_values.append(st.session_state.provider_features.get('avg_reimb', ranges[feature]['default']) * 0.8)
            elif feature == 'std_reimb':
                feature_values.append(st.session_state.provider_features.get('avg_reimb', ranges[feature]['default']) * 1.5)
            else:
                feature_values.append(ranges[feature]['default'])
        else:
            feature_values.append(0)  # Default for missing features
        feature_names.append(feature)
    
    # Make prediction
    try:
        import numpy as np
        feature_array = np.array([feature_values])
        fraud_probability = model.predict_proba(feature_array)[0][1]
        
        # Determine risk level and color
        if fraud_probability >= 0.7:
            risk_level = "HIGH RISK"
            risk_color = "#dc3545"
            risk_icon = "ğŸš¨"
            recommendation = "IMMEDIATE INVESTIGATION RECOMMENDED"
            rec_color = "#721c24"
        elif fraud_probability >= 0.4:
            risk_level = "MODERATE RISK"
            risk_color = "#fd7e14" 
            risk_icon = "âš ï¸"
            recommendation = "ENHANCED MONITORING SUGGESTED"
            rec_color = "#495057"
        else:
            risk_level = "LOW RISK"
            risk_color = "#28a745"
            risk_icon = "âœ…"
            recommendation = "ROUTINE OVERSIGHT SUFFICIENT"
            rec_color = "#155724"
        
        # Display prediction results
        st.markdown("---")
        st.subheader("ğŸ“Š Fraud Risk Assessment Results")
        
        # Main prediction display
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, {risk_color} 0%, {risk_color}dd 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0; text-align: center;">
            <h2 style="margin: 0 0 1rem 0;">{risk_icon} {risk_level}</h2>
            <h1 style="margin: 0; font-size: 3rem;">{fraud_probability:.1%}</h1>
            <p style="margin: 0.5rem 0 0 0; font-size: 1.2rem;">Fraud Probability</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"""
        <div style="background-color: {rec_color}; color: white; padding: 1rem; border-radius: 10px; text-align: center; margin: 1rem 0;">
            <h4 style="margin: 0;">ğŸ“‹ Recommendation: {recommendation}</h4>
        </div>
        """, unsafe_allow_html=True)
        
        # Risk factor analysis
        st.subheader("ğŸ” Risk Factor Analysis")
        
        # Get feature importance for explanation
        feature_impacts = []
        for i, feature in enumerate(all_features):
            value = feature_values[i]
            if feature in ranges:
                # Calculate relative position (0 = min, 1 = max)
                range_pos = (value - ranges[feature]['min']) / max(ranges[feature]['max'] - ranges[feature]['min'], 1)
                
                # Determine impact level
                if range_pos > 0.8:
                    impact = "Very High"
                    impact_color = "#dc3545"
                elif range_pos > 0.6:
                    impact = "High" 
                    impact_color = "#fd7e14"
                elif range_pos > 0.4:
                    impact = "Moderate"
                    impact_color = "#ffc107"
                elif range_pos > 0.2:
                    impact = "Low"
                    impact_color = "#28a745"
                else:
                    impact = "Very Low"
                    impact_color = "#6c757d"
                
                if feature in feature_definitions:
                    feature_impacts.append({
                        'feature': feature_definitions[feature]['name'],
                        'value': value,
                        'impact': impact,
                        'color': impact_color,
                        'range_pos': range_pos,
                        'format': feature_definitions[feature]['format']
                    })
        
        # Sort by impact level
        feature_impacts.sort(key=lambda x: x['range_pos'], reverse=True)
        
        # Display top risk factors
        risk_col1, risk_col2 = st.columns(2)
        
        with risk_col1:
            st.markdown("#### ğŸ”´ High Risk Indicators")
            high_risk_found = False
            for impact in feature_impacts[:8]:
                if impact['range_pos'] > 0.6:
                    high_risk_found = True
                    if impact['format'] == 'currency':
                        value_str = f"${impact['value']:,.0f}"
                    elif impact['format'] == 'percent':
                        value_str = f"{impact['value']:.1%}"
                    elif impact['format'] == 'int':
                        value_str = f"{impact['value']:.0f}"
                    else:
                        value_str = f"{impact['value']:.2f}"
                    
                    st.markdown(f"""
                    <div style="background-color: {impact['color']}22; border-left: 4px solid {impact['color']}; padding: 0.5rem; margin: 0.5rem 0; border-radius: 5px;">
                        <strong>{impact['feature']}:</strong> {value_str} <span style="color: {impact['color']};">({impact['impact']})</span>
                    </div>
                    """, unsafe_allow_html=True)
            
            if not high_risk_found:
                st.markdown("*No significant high-risk indicators detected*")
        
        with risk_col2:
            st.markdown("#### ğŸŸ¢ Protective Factors")
            protective_found = False
            for impact in feature_impacts:
                if impact['range_pos'] <= 0.4:
                    protective_found = True
                    if impact['format'] == 'currency':
                        value_str = f"${impact['value']:,.0f}"
                    elif impact['format'] == 'percent':
                        value_str = f"{impact['value']:.1%}"
                    elif impact['format'] == 'int':
                        value_str = f"{impact['value']:.0f}"
                    else:
                        value_str = f"{impact['value']:.2f}"
                    
                    st.markdown(f"""
                    <div style="background-color: {impact['color']}22; border-left: 4px solid {impact['color']}; padding: 0.5rem; margin: 0.5rem 0; border-radius: 5px;">
                        <strong>{impact['feature']}:</strong> {value_str} <span style="color: {impact['color']};">({impact['impact']})</span>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if len([f for f in feature_impacts if f['range_pos'] <= 0.4]) >= 4:
                        break
            
            if not protective_found:
                st.markdown("*No significant protective factors identified*")
        
        # Business interpretation
        st.subheader("ğŸ’¼ Business Intelligence Summary")
        
        interpretation_text = []
        
        if fraud_probability >= 0.7:
            interpretation_text.append("ğŸš¨ **CRITICAL ALERT**: This provider profile exhibits multiple characteristics strongly associated with fraudulent activity.")
        elif fraud_probability >= 0.4:
            interpretation_text.append("âš ï¸ **ELEVATED CONCERN**: This provider shows several patterns that warrant closer examination.")
        else:
            interpretation_text.append("âœ… **ACCEPTABLE RISK**: This provider profile appears consistent with legitimate healthcare practices.")
        
        # Add specific insights based on feature values
        total_reimb = st.session_state.provider_features.get('total_reimb', 0)
        if total_reimb > 200000:
            interpretation_text.append(f"â€¢ Total reimbursement of ${total_reimb:,.0f} is significantly above typical provider levels")
        
        kidney_rate = st.session_state.provider_features.get('kidney_rate', 0)
        if kidney_rate > 0.7:
            interpretation_text.append(f"â€¢ Kidney disease rate of {kidney_rate:.1%} suggests potential patient mix manipulation")
        
        claims_per_bene = st.session_state.provider_features.get('claims_per_bene', 1)
        if claims_per_bene > 2.5:
            interpretation_text.append(f"â€¢ {claims_per_bene:.1f} claims per patient indicates possible over-utilization")
        
        weekend_pct = st.session_state.provider_features.get('pct_weekend', 0.25)
        if weekend_pct > 0.4:
            interpretation_text.append(f"â€¢ {weekend_pct:.1%} weekend claims is unusually high for most specialties")
        
        for text in interpretation_text:
            st.markdown(text)
        
        # Investigation guidance
        if fraud_probability >= 0.4:
            st.markdown("#### ğŸ” Recommended Investigation Focus Areas")
            
            guidance = []
            if total_reimb > 100000:
                guidance.append("â€¢ **Financial Review**: Examine high-value claims for appropriate coding and documentation")
            if kidney_rate > 0.6 or st.session_state.provider_features.get('heartfail_rate', 0) > 0.8:
                guidance.append("â€¢ **Patient Records**: Verify chronic condition diagnoses and treatment appropriateness") 
            if claims_per_bene > 2:
                guidance.append("â€¢ **Utilization Analysis**: Review necessity and frequency of services provided")
            if weekend_pct > 0.35:
                guidance.append("â€¢ **Billing Patterns**: Investigate unusual timing of service delivery and claims submission")
            
            for item in guidance:
                st.markdown(item)
    
    except Exception as e:
        st.error(f"âŒ Prediction error: {e}")
        st.info("Please ensure all feature values are properly set and the model is loaded correctly.")

with tab6:
    st.header("ğŸ’¼ Business Impact & Value")
    
    # Load business impact data
    @st.cache_data
    def load_business_data():
        """Load business impact metrics and calculations"""
        business_data = {}
        
        try:
            # Load EDA metrics for financial calculations
            eda_metrics_path = DATA_DIR / "eda" / "eda_metrics.json"
            if eda_metrics_path.exists():
                with open(eda_metrics_path, 'r') as f:
                    business_data['eda_metrics'] = json.load(f)
            
            # Load model performance metrics
            model_metrics_path = DATA_DIR / "metrics" / "XGBost_metrics.json"
            if model_metrics_path.exists():
                with open(model_metrics_path, 'r') as f:
                    business_data['model_metrics'] = json.load(f)
            
            # Load feature statistics
            feature_stats_path = DATA_DIR / "features_stats.csv"
            if feature_stats_path.exists():
                business_data['feature_stats'] = pd.read_csv(feature_stats_path, index_col=0)
            
            # Load validation data
            validation_path = DATA_DIR / "validation_sets" / "X_val_full.parquet"
            if validation_path.exists():
                business_data['validation_data'] = pd.read_parquet(validation_path)
                
        except Exception as e:
            st.error(f"Error loading business data: {e}")
        
        return business_data
    
    business_data = load_business_data()
    
    if not business_data:
        st.error("âŒ Business impact data not available. Please ensure all required files are loaded.")
        st.stop()
    
    # Executive Summary Header
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0;">
        <h3 style="margin: 0 0 1rem 0;">ğŸ¯ Business Value: How This System Helps Your Organization</h3>
        <p style="margin: 0; font-size: 1.1rem;">Real impact metrics from our fraud detection analysis - demonstrating concrete value for business stakeholders and investigation teams.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Calculate key business metrics from real data
    if 'eda_metrics' in business_data and 'model_metrics' in business_data:
        eda = business_data['eda_metrics']
        model = business_data['model_metrics']
        
        # Real data calculations
        fraud_rate = eda['fraud_percent'] / 100  # 9.35%
        avg_fraud_amount = eda['avg_reimb_fraud']  # $584,350
        avg_legit_amount = eda['avg_reimb_legit']  # $53,194
        
        # Model performance from actual results
        model_precision_fraud = model['precision_fraud']  # 53%
        model_recall_fraud = model['recall_fraud']  # 76%
        model_auc = model['auc']  # 94.7%
        
        # Business calculations based on our dataset
        total_providers = 5410  # From our actual dataset
        total_fraud_cases = int(total_providers * fraud_rate)
        detected_fraud_cases = int(total_fraud_cases * model_recall_fraud)
        prevented_losses = detected_fraud_cases * avg_fraud_amount
        
        # Key Impact Metrics Dashboard
        st.subheader("ğŸ“Š Key Value Metrics from Our Analysis")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #28a745 0%, #20c997 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ’° Fraud Prevented</h4>
                <h2 style="margin: 0.5rem 0;">${prevented_losses/1000000:.1f}M</h2>
                <p style="margin: 0; font-size: 0.9rem;">Annual fraud losses prevented</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #007bff 0%, #0056b3 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ¯ Detection Rate</h4>
                <h2 style="margin: 0.5rem 0;">{model_recall_fraud*100:.0f}%</h2>
                <p style="margin: 0; font-size: 0.9rem;">Of fraud cases detected</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #fd7e14 0%, #e55a4e 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">âš¡ Model Accuracy</h4>
                <h2 style="margin: 0.5rem 0;">{model_auc*100:.1f}%</h2>
                <p style="margin: 0; font-size: 0.9rem;">AUC Performance Score</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            investigation_precision = model_precision_fraud * 100
            st.markdown(f"""
            <div style="background: linear-gradient(135deg, #6f42c1 0%, #563d7c 100%); padding: 1.5rem; border-radius: 10px; color: white; text-align: center;">
                <h4 style="margin: 0;">ğŸ¯ Hit Rate</h4>
                <h2 style="margin: 0.5rem 0;">{investigation_precision:.0f}%</h2>
                <p style="margin: 0; font-size: 0.9rem;">Fraud Alert Accuracy</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Value Proposition for Different Stakeholders
        st.subheader("ğŸ¢ Value for Different Teams & Stakeholders")
        
        # Fraud Investigation Teams
        st.markdown("#### ğŸ•µï¸ For Fraud Investigation Teams")
        
        investigation_col1, investigation_col2 = st.columns(2)
        
        with investigation_col1:
            st.markdown(f"""
            <div style="background-color: #e8f4fd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
            <h5>ğŸ¯ Improved Investigation Efficiency</h5>
            <ul>
                <li><strong>Focus on High-Value Cases:</strong> Average fraudulent provider costs ${avg_fraud_amount:,} vs ${avg_legit_amount:,} for legitimate</li>
                <li><strong>Better Hit Rate:</strong> {model_precision_fraud*100:.0f}% of flagged cases are actual fraud</li>
                <li><strong>Comprehensive Coverage:</strong> Detects {model_recall_fraud*100:.0f}% of fraud cases automatically</li>
                <li><strong>Clear Explanations:</strong> SHAP analysis shows exactly why providers are flagged</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with investigation_col2:
            st.markdown(f"""
            <div style="background-color: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #6c757d;">
            <h5>âš¡ Daily Workflow Benefits</h5>
            <ul>
                <li><strong>Prioritized Case List:</strong> Start with highest-risk providers first</li>
                <li><strong>Supporting Evidence:</strong> Feature analysis shows key risk indicators</li>
                <li><strong>Time Savings:</strong> No need to manually screen {total_providers:,} providers</li>
                <li><strong>Documentation:</strong> AI explanations support audit findings</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # Management & Leadership
        st.markdown("#### ğŸ‘” For Management & Leadership")
        
        mgmt_col1, mgmt_col2 = st.columns(2)
        
        with mgmt_col1:
            st.markdown(f"""
            <div style="background-color: #d4edda; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
            <h5>ğŸ’° Financial Impact</h5>
            <ul>
                <li><strong>Fraud Prevention:</strong> ${prevented_losses/1000000:.1f}M annually in detected fraud</li>
                <li><strong>Resource Optimization:</strong> Focus investigations on {detected_fraud_cases} high-probability cases</li>
                <li><strong>Risk Mitigation:</strong> Catches {model_recall_fraud*100:.0f}% of fraudulent activity</li>
                <li><strong>Scalable Solution:</strong> Monitors all {total_providers:,} providers continuously</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with mgmt_col2:
            st.markdown(f"""
            <div style="background-color: #fff3cd; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #ffc107;">
            <h5>ğŸ“Š Operational Excellence</h5>
            <ul>
                <li><strong>Data-Driven Decisions:</strong> {model_auc*100:.1f}% accuracy provides confidence</li>
                <li><strong>Regulatory Compliance:</strong> Explainable AI supports audit requirements</li>
                <li><strong>Performance Monitoring:</strong> Track fraud detection trends over time</li>
                <li><strong>Preventive Effect:</strong> Visible monitoring deters potential fraud</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # Data Science & Analytics Teams
        st.markdown("#### ğŸ“ˆ For Data Science & Analytics Teams")
        
        ds_col1, ds_col2 = st.columns(2)
        
        with ds_col1:
            st.markdown(f"""
            <div style="background-color: #f3e5f5; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #9c27b0;">
            <h5>ğŸ”¬ Technical Achievement</h5>
            <ul>
                <li><strong>Model Performance:</strong> AUC of {model_auc:.3f} indicates excellent discrimination</li>
                <li><strong>Balanced Metrics:</strong> Precision {model_precision_fraud:.2f}, Recall {model_recall_fraud:.2f}</li>
                <li><strong>Feature Engineering:</strong> 28 carefully selected features from healthcare data</li>
                <li><strong>Explainable AI:</strong> SHAP values provide transparent decision-making</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with ds_col2:
            st.markdown(f"""
            <div style="background-color: #e8f5e8; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #4caf50;">
            <h5>ğŸ¯ Business Translation</h5>
            <ul>
                <li><strong>Measurable Impact:</strong> Quantified fraud prevention in dollar terms</li>
                <li><strong>Stakeholder Communication:</strong> Technical metrics translated to business value</li>
                <li><strong>Continuous Improvement:</strong> Performance monitoring and model refinement</li>
                <li><strong>Real-World Application:</strong> Production-ready fraud detection system</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # Financial Impact Visualization
        st.subheader("ğŸ’° Financial Impact Analysis")
        
        # Create comparison chart
        comparison_data = {
            'Provider Type': ['Fraudulent Providers', 'Legitimate Providers'],
            'Average Reimbursement': [avg_fraud_amount, avg_legit_amount],
            'Count in Dataset': [total_fraud_cases, total_providers - total_fraud_cases]
        }
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='Average Reimbursement',
            x=comparison_data['Provider Type'],
            y=comparison_data['Average Reimbursement'],
            text=[f'${x:,.0f}' for x in comparison_data['Average Reimbursement']],
            textposition='auto',
            marker_color=['#dc3545', '#28a745']
        ))
        
        fig.update_layout(
            title="Average Reimbursement: Fraudulent vs Legitimate Providers",
            yaxis_title="Average Reimbursement ($)",
            height=400,
            showlegend=False
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Model Performance Business Translation
        st.subheader("ğŸ“ˆ How Model Performance Translates to Business Value")
        
        perf_col1, perf_col2 = st.columns(2)
        
        with perf_col1:
            st.markdown(f"""
            #### ğŸ¯ What Our Metrics Mean
            
            **Precision ({model_precision_fraud*100:.0f}%):**
            - When we flag a provider as fraudulent, we're right {model_precision_fraud*100:.0f}% of the time
            - Reduces wasted investigation effort on false alarms
            - **Value:** Investigators can trust the system's recommendations
            
            **Recall ({model_recall_fraud*100:.0f}%):**
            - We catch {model_recall_fraud*100:.0f}% of all actual fraud cases
            - {100-model_recall_fraud*100:.0f}% of fraud might initially be missed but can be caught in secondary reviews
            - **Value:** Comprehensive fraud detection with acceptable coverage
            
            **AUC ({model_auc*100:.1f}%):**
            - Excellent ability to distinguish between fraud and legitimate activity
            - Industry-leading performance for healthcare fraud detection
            - **Value:** Reliable, trustworthy decision support
            """)
        
        with perf_col2:
            st.markdown(f"""
            #### ğŸ’¼ Real-World Impact
            
            **Investigation Workload:**
            - Instead of reviewing all {total_providers:,} providers manually
            - Focus on {int(total_providers * fraud_rate / model_precision_fraud):,} flagged cases with {model_precision_fraud*100:.0f}% accuracy
            - **Result:** More efficient use of investigation resources
            
            **Fraud Prevention:**
            - Detect {detected_fraud_cases} out of {total_fraud_cases} fraud cases
            - Prevent ${prevented_losses/1000000:.1f}M in fraudulent reimbursements
            - **Result:** Direct financial protection for the organization
            
            **Risk Management:**
            - Continuous monitoring of entire provider population
            - Early detection before fraud escalates
            - **Result:** Proactive rather than reactive fraud management
            """)
        
        # Success Factors for Implementation
        st.subheader("âœ… Critical Success Factors")
        
        success_col1, success_col2 = st.columns(2)
        
        with success_col1:
            st.markdown("""
            <div style="background-color: #d1ecf1; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #17a2b8;">
            <h5>ğŸ”‘ Technical Requirements</h5>
            <ul>
                <li><strong>Data Quality:</strong> Consistent, clean healthcare reimbursement data</li>
                <li><strong>Feature Pipeline:</strong> Automated calculation of risk indicators</li>
                <li><strong>Model Monitoring:</strong> Track performance and detect drift over time</li>
                <li><strong>Integration:</strong> Connect with existing investigation workflows</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        with success_col2:
            st.markdown("""
            <div style="background-color: #d4edda; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
            <h5>ğŸ‘¥ Organizational Requirements</h5>
            <ul>
                <li><strong>User Training:</strong> Educate teams on AI-assisted investigations</li>
                <li><strong>Process Integration:</strong> Incorporate model outputs into existing workflows</li>
                <li><strong>Performance Tracking:</strong> Monitor business impact and adjust thresholds</li>
                <li><strong>Feedback Loop:</strong> Use investigation results to improve model accuracy</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # Key Takeaways
        st.subheader("ğŸ¯ Key Takeaways")
        
        st.markdown(f"""
        <div style="background: linear-gradient(135deg, #343a40 0%, #495057 100%); padding: 2rem; border-radius: 15px; color: white; margin: 1rem 0;">
            <h4 style="margin: 0 0 1rem 0;">ğŸ’¡ Bottom Line Business Value</h4>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin-top: 1rem;">
                <div>
                    <p style="margin: 0.5rem 0;"><strong>ğŸ¯ For Investigators:</strong> Focus on {detected_fraud_cases} high-probability cases instead of reviewing all {total_providers:,} providers</p>
                    <p style="margin: 0.5rem 0;"><strong>ğŸ’° For Leadership:</strong> Prevent ${prevented_losses/1000000:.1f}M in annual fraud losses with {model_auc*100:.1f}% model accuracy</p>
                </div>
                <div>
                    <p style="margin: 0.5rem 0;"><strong>ğŸ“Š For Analytics Teams:</strong> Proven ML system with explainable results and measurable business impact</p>
                    <p style="margin: 0.5rem 0;"><strong>âš–ï¸ For Compliance:</strong> Transparent, auditable fraud detection with documented decision rationale</p>
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    else:
        st.error("âŒ Unable to load business metrics. Please ensure all data files are available.")
